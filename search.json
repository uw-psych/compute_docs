[
  {
    "objectID": "docs/CONTRIBUTING.html",
    "href": "docs/CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "Contributions are welcome and appreciated! Every little bit helps and we will always give you credit. You can contribute in the ways listed below.\n\n\nReport errors and other issues using the ‚ÄúReport issue‚Äù link at the bottom of each page or by going to the issues page on GitHub.\nIf you are reporting an issue related to running the code, please include the following information:\n\nYour operating system name and version\nWhether you are running the code on Klone, and if you are:\n\nWhether you are running the code on a login node or a compute node\n\nAny details about your local setup that might be helpful in troubleshooting\nDetailed steps to reproduce the issue\n\n\n\n\nThe best way to send feedback is to file an issue on GitHub.\nIf you are proposing a feature or change, please:\n\nExplain in detail how it would work or what it would cover\nKeep the scope as narrow as possible\n\n\n\n\nHave a question or want to discuss something? There are a few options:\n\nüì• File an issue on GitHub\nüßµ Start a discussion or read past discussions on GitHub Discussions\nüìß Send an e-mail to the psych-hpc mailing list or read the archives\n\n\n\n\nYou can contribute content or changes to the guide by forking this repository and submitting a pull request.\n\n\nTo contribute to this guide, you should be familiar with collaborating on GitHub, using Git, and writing documents in Markdown (specifically Quarto Markdown).\n\nFork this repository.\nClone your fork locally.\nCreate a new branch for your changes.\nIf you want to preview or build the site locally:\n\nInstall Quarto (use version 1.4 or higher, or a 1.4 pre-release version).\nInstall R (v4.1 or higher) and install the rmarkdown package with install.packages(\"rmarkdown\").\n\nMake your changes (generally to .qmd files).\nAdd yourself to the list of authors in the YAML front matter of this page.\nIf adding a new page:\n\nAdd a link to the page in the _quarto.yml sidebar section.\nIf the page is part of a folder that contains an index.qmd file that has a listing section and the contents section within it does not include all the pages using a wildcard (e.g., - \"*.qmd\"), add the page to the contents section under the listing section.\nIf the page should be listed on the Welcome page, add a link to the page in the index.qmd file at the root of the repository.\nEnsure that the page has a description in the YAML front matter.\nEnsure that the page has a title in the YAML front matter OR that the page has a # heading at the top of the page.\n\nIf possible, preview your changes locally using quarto preview.\nOpen a pull request.\n\nOnce the pull request is opened, a project maintainer will review your changes and may request changes or ask questions. Once the changes are approved, a project maintainer will merge your changes into the main repository.\n\n\n\nThe repository is structured as a Quarto website project.\nThe main files and folders are:\n\n_quarto.yml: Quarto configuration file for the website\nindex.qmd: Quarto Markdown file for the home page\ndocs/: contains the page content\n_examples/: contains example code\n_static: contains static files that will be available to published web pages (e.g., images, CSS, JavaScript)\n_extensions/: contains installed Quarto extensions\n\n\n\n\n\nThis work is licensed under the MIT license",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/CONTRIBUTING.html#report-issues",
    "href": "docs/CONTRIBUTING.html#report-issues",
    "title": "Contributing",
    "section": "",
    "text": "Report errors and other issues using the ‚ÄúReport issue‚Äù link at the bottom of each page or by going to the issues page on GitHub.\nIf you are reporting an issue related to running the code, please include the following information:\n\nYour operating system name and version\nWhether you are running the code on Klone, and if you are:\n\nWhether you are running the code on a login node or a compute node\n\nAny details about your local setup that might be helpful in troubleshooting\nDetailed steps to reproduce the issue",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/CONTRIBUTING.html#submitting-feedback",
    "href": "docs/CONTRIBUTING.html#submitting-feedback",
    "title": "Contributing",
    "section": "",
    "text": "The best way to send feedback is to file an issue on GitHub.\nIf you are proposing a feature or change, please:\n\nExplain in detail how it would work or what it would cover\nKeep the scope as narrow as possible",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/CONTRIBUTING.html#questions-and-discussion",
    "href": "docs/CONTRIBUTING.html#questions-and-discussion",
    "title": "Contributing",
    "section": "",
    "text": "Have a question or want to discuss something? There are a few options:\n\nüì• File an issue on GitHub\nüßµ Start a discussion or read past discussions on GitHub Discussions\nüìß Send an e-mail to the psych-hpc mailing list or read the archives",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/CONTRIBUTING.html#contributing-new-content-or-changes",
    "href": "docs/CONTRIBUTING.html#contributing-new-content-or-changes",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute content or changes to the guide by forking this repository and submitting a pull request.\n\n\nTo contribute to this guide, you should be familiar with collaborating on GitHub, using Git, and writing documents in Markdown (specifically Quarto Markdown).\n\nFork this repository.\nClone your fork locally.\nCreate a new branch for your changes.\nIf you want to preview or build the site locally:\n\nInstall Quarto (use version 1.4 or higher, or a 1.4 pre-release version).\nInstall R (v4.1 or higher) and install the rmarkdown package with install.packages(\"rmarkdown\").\n\nMake your changes (generally to .qmd files).\nAdd yourself to the list of authors in the YAML front matter of this page.\nIf adding a new page:\n\nAdd a link to the page in the _quarto.yml sidebar section.\nIf the page is part of a folder that contains an index.qmd file that has a listing section and the contents section within it does not include all the pages using a wildcard (e.g., - \"*.qmd\"), add the page to the contents section under the listing section.\nIf the page should be listed on the Welcome page, add a link to the page in the index.qmd file at the root of the repository.\nEnsure that the page has a description in the YAML front matter.\nEnsure that the page has a title in the YAML front matter OR that the page has a # heading at the top of the page.\n\nIf possible, preview your changes locally using quarto preview.\nOpen a pull request.\n\nOnce the pull request is opened, a project maintainer will review your changes and may request changes or ask questions. Once the changes are approved, a project maintainer will merge your changes into the main repository.\n\n\n\nThe repository is structured as a Quarto website project.\nThe main files and folders are:\n\n_quarto.yml: Quarto configuration file for the website\nindex.qmd: Quarto Markdown file for the home page\ndocs/: contains the page content\n_examples/: contains example code\n_static: contains static files that will be available to published web pages (e.g., images, CSS, JavaScript)\n_extensions/: contains installed Quarto extensions",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/CONTRIBUTING.html#licensing",
    "href": "docs/CONTRIBUTING.html#licensing",
    "title": "Contributing",
    "section": "",
    "text": "This work is licensed under the MIT license",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/compute/index.html",
    "href": "docs/compute/index.html",
    "title": "Using the cluster",
    "section": "",
    "text": "UW Hyak clusters use SLURM to manage access to compute resources and schedule compute jobs.",
    "crumbs": [
      "Using the cluster"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html",
    "href": "docs/compute/lmod.html",
    "title": "Lmod",
    "section": "",
    "text": "Lmod is a module system that is used to load additional software on Klone compute nodes.\nLmod is run with the command module. This command is only available on compute nodes and will not run on the login node, so you need to launch a SLURM job on Klone to run the command.\n\n\nmodule help\n\n\n\nmodule avail\n\n\n\nmodule load &lt;module_name&gt; # To load the default version of a module\nmodule load &lt;module_name&gt;/&lt;version&gt; # To load a specific version of a module\nIf the version is not specified, Lmod will load a default version marked with D.\n\n\n\nmodule help &lt;module&gt;\n\n\n\nmodule list\n\n\n\nmodule unload &lt;module&gt;\n\n\n\nmodule purge\n\n\n\nmodule whatis &lt;module&gt;",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#show-help-for-lmod",
    "href": "docs/compute/lmod.html#show-help-for-lmod",
    "title": "Lmod",
    "section": "",
    "text": "module help",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#checking-all-available-modules",
    "href": "docs/compute/lmod.html#checking-all-available-modules",
    "title": "Lmod",
    "section": "",
    "text": "module avail",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#load-a-module",
    "href": "docs/compute/lmod.html#load-a-module",
    "title": "Lmod",
    "section": "",
    "text": "module load &lt;module_name&gt; # To load the default version of a module\nmodule load &lt;module_name&gt;/&lt;version&gt; # To load a specific version of a module\nIf the version is not specified, Lmod will load a default version marked with D.",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#show-help-for-a-module",
    "href": "docs/compute/lmod.html#show-help-for-a-module",
    "title": "Lmod",
    "section": "",
    "text": "module help &lt;module&gt;",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#list-loaded-modules",
    "href": "docs/compute/lmod.html#list-loaded-modules",
    "title": "Lmod",
    "section": "",
    "text": "module list",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#unload-a-module",
    "href": "docs/compute/lmod.html#unload-a-module",
    "title": "Lmod",
    "section": "",
    "text": "module unload &lt;module&gt;",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#unload-all-modules",
    "href": "docs/compute/lmod.html#unload-all-modules",
    "title": "Lmod",
    "section": "",
    "text": "module purge",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#show-module-description",
    "href": "docs/compute/lmod.html#show-module-description",
    "title": "Lmod",
    "section": "",
    "text": "module whatis &lt;module&gt;",
    "crumbs": [
      "Using the cluster",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/software/conn.html",
    "href": "docs/software/conn.html",
    "title": "CONN Toolbox",
    "section": "",
    "text": "The Functional Connectivity (CONN)) toolbox for MATLAB is available as an Lmod module on Klone compute nodes.\n\n\nLike all Lmod modules, the conn module is available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the module for CONN is installed in the escience hierarchy, its name is prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/conn\nThe output should look a bit like this:\n---------------------------------------\n  escience/conn: escience/conn/v.22.a\n---------------------------------------\n\n    This module can be loaded directly: module load escience/conn/v.22.a\n\n    Help:\n      conn-v.22.a\n\n\n\nLoad the default version with module load escience/conn or load a specific version with module load escience/conn/version.\nAfter loading the module, start matlab and run conn in the MATLAB command window.\n\n\n\nYou can install different versions of CONN by following the instructions below.\n\nGo to https://www.nitrc.org/frs/?group_id=279, choose the version of CONN you want to install, copy the download link to the file, and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 21a as an example:\n1mkdir -p \"/gscratch/scrubbed/$USER/downloads\" && cd \"$_\"\n2curl -LO https://www.nitrc.org/frs/download.php/12426/conn21a.zip\n\n1\n\nCreate the target directory if it does not exist and navigate to it ($_ is a special variable that expands to the last argument of the previous command).\n\n2\n\ncurl is a program you can use to download files from the web. Replace the URL with the URL for the version of CONN you want to install. The -L option tells curl to follow redirects, and the -O option tells it to save the file to the current directory.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe directory /gscratch/scrubbed/$USER/downloads will expand to /gscratch/scrubbed/(your username)/downloads. This directory is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the archive and copy the contents to /sw/contrib/mylabname-src/conn/21a (replacing mylabname with the name of your lab):\n1unzip conn21a.zip -d conn21a.zip\n2# Make sure you include the parentheses around the commands below. Replace \"mylabname\" with your lab name!\n3(umask 002 && mkdir -p /sw/contrib/mylabname-src/conn/21a && rsync -rlHP --chmod=a+rwX conn21a/conn/ $_)\n4rm -rfv conn21a.zip conn21a\n\n1\n\nExtract the CONN archive to a new directory called conn21a.\n\n2\n\nIn bash, parentheses create a [subshell]((https://tldp.org/LDP/abs/html/subshells.html), which runs the enclosed commands in a new shell session. We want to use it here so that we can temporarily modify the default permissions for new files and directories with the umask command. Your current settings will remain intact when the commands in the subshell are completed.\n\n3\n\nWe‚Äôre running a series of commands here in a [subshell]((https://tldp.org/LDP/abs/html/subshells.html). Let‚Äôs break it down:\n\n4\n\nRemove the downloaded file and the extracted directory.\n\n\nUse a text editor to create an Lmod .lua module file for the new release with a text editor, using 21a.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/conn21a.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\ndepends_on(\"matlab\")\ndepends_on(\"escience/spm\") -- edit this line to match the name of your SPM module\nappend_path(\"MATLABPATH\", base)\n\n\n\n\n\n\n\nTip\n\n\n\nIf you don‚Äôt know how to use a text editor on Klone, you can try nano, which is installed on the system. To create a new file, run nano filename, paste the contents of the template into the file, and save it by pressing Ctrl+O and then Enter. To exit, press Ctrl+X.\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/conn\nmodule load mylabname/conn/21a\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.",
    "crumbs": [
      "Software",
      "CONN Toolbox"
    ]
  },
  {
    "objectID": "docs/software/conn.html#use-with-lmod",
    "href": "docs/software/conn.html#use-with-lmod",
    "title": "CONN Toolbox",
    "section": "",
    "text": "Like all Lmod modules, the conn module is available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the module for CONN is installed in the escience hierarchy, its name is prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/conn\nThe output should look a bit like this:\n---------------------------------------\n  escience/conn: escience/conn/v.22.a\n---------------------------------------\n\n    This module can be loaded directly: module load escience/conn/v.22.a\n\n    Help:\n      conn-v.22.a\n\n\n\nLoad the default version with module load escience/conn or load a specific version with module load escience/conn/version.\nAfter loading the module, start matlab and run conn in the MATLAB command window.\n\n\n\nYou can install different versions of CONN by following the instructions below.\n\nGo to https://www.nitrc.org/frs/?group_id=279, choose the version of CONN you want to install, copy the download link to the file, and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 21a as an example:\n1mkdir -p \"/gscratch/scrubbed/$USER/downloads\" && cd \"$_\"\n2curl -LO https://www.nitrc.org/frs/download.php/12426/conn21a.zip\n\n1\n\nCreate the target directory if it does not exist and navigate to it ($_ is a special variable that expands to the last argument of the previous command).\n\n2\n\ncurl is a program you can use to download files from the web. Replace the URL with the URL for the version of CONN you want to install. The -L option tells curl to follow redirects, and the -O option tells it to save the file to the current directory.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe directory /gscratch/scrubbed/$USER/downloads will expand to /gscratch/scrubbed/(your username)/downloads. This directory is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the archive and copy the contents to /sw/contrib/mylabname-src/conn/21a (replacing mylabname with the name of your lab):\n1unzip conn21a.zip -d conn21a.zip\n2# Make sure you include the parentheses around the commands below. Replace \"mylabname\" with your lab name!\n3(umask 002 && mkdir -p /sw/contrib/mylabname-src/conn/21a && rsync -rlHP --chmod=a+rwX conn21a/conn/ $_)\n4rm -rfv conn21a.zip conn21a\n\n1\n\nExtract the CONN archive to a new directory called conn21a.\n\n2\n\nIn bash, parentheses create a [subshell]((https://tldp.org/LDP/abs/html/subshells.html), which runs the enclosed commands in a new shell session. We want to use it here so that we can temporarily modify the default permissions for new files and directories with the umask command. Your current settings will remain intact when the commands in the subshell are completed.\n\n3\n\nWe‚Äôre running a series of commands here in a [subshell]((https://tldp.org/LDP/abs/html/subshells.html). Let‚Äôs break it down:\n\n4\n\nRemove the downloaded file and the extracted directory.\n\n\nUse a text editor to create an Lmod .lua module file for the new release with a text editor, using 21a.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/conn21a.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\ndepends_on(\"matlab\")\ndepends_on(\"escience/spm\") -- edit this line to match the name of your SPM module\nappend_path(\"MATLABPATH\", base)\n\n\n\n\n\n\n\nTip\n\n\n\nIf you don‚Äôt know how to use a text editor on Klone, you can try nano, which is installed on the system. To create a new file, run nano filename, paste the contents of the template into the file, and save it by pressing Ctrl+O and then Enter. To exit, press Ctrl+X.\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/conn\nmodule load mylabname/conn/21a\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.",
    "crumbs": [
      "Software",
      "CONN Toolbox"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html",
    "href": "docs/software/freesurfer.html",
    "title": "FreeSurfer",
    "section": "",
    "text": "FreeSurfer is an open-source neuroimaging toolkit available as a Lmod module on Klone.\n\n\n\n\n\n\nImportant\n\n\n\nCurrently, FreeSurfer must run from a container with sufficient library dependencies. We recommend using FreeSurfer from a VNC session with hyakvnc.\n\n\n\n\nmodule avail freesurfer\n\n\n\nLoad the default version with module load escience/freesurfer,\nor load a specific version with module load escience/freesurfer/&lt;version&gt;.\nAfter loading the module, FreeSurfer should have its environment set up for normal use.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is tested from a VNC Apptainer environment with sufficient dependencies.\n\n\n\n\nGo to https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall and download the latest FreeSurfer tar archive for CentOS 8\nExtract the .tar.gz archive to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/.\n\nFor example, if installing version 7.3.2 of FreeSurfer, then run the following:\nLABNAME=escience\nVERSION=7.3.2\nmkdir -p \"/sw/contrib/$LABNAME-src/freesurfer/$VERSION\"\ntar -xzf freesurfer-linux-centos8_x86_64-$VERSION.tar.gz \\\n    --strip-components=1 \\\n    -C \"/sw/contrib/$LABNAME-src/freesurfer/$VERSION\"\n\nCreate an Lmod .lua module file for the new release with a text editor:\n\n\n\n/sw/contrib/modulefiles/mylabname/freesurfer/7.3.2.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\n--FreeSurfer 7.x requires MATLAB R2014b\ndepends_on(\"forsyth/matlab/r2014b\")\n\nsetenv(\"FREESURFER_HOME\", base)\nprepend_path(\"PATH\", pathJoin(base, \"bin\"))\nsource_sh(\"bash\", pathJoin(base, \"SetUpFreeSurfer.sh\"))\n\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/freesurfer\nmodule load mylabname/freesurfer/7.3.2\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.\n\n\nRequest a free license from https://surfer.nmr.mgh.harvard.edu/registration.html if not already.\n\n\n\n\n\n\n\nNote\n\n\n\nThis license is not tied to a specific version of FreeSurfer and can be copied from a previous installation.\n\n\n\nDownload license.txt from the registration email and copy it to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/license.txt.\nFor FS-FAST support, create a link to a supported MATLAB release (R2014b for FreeSurfer 7.x):\n\nmodule -I load labname/freesurfer/&lt;version&gt;\ncd \"$FREESURFER_HOME\"\nln -s /gscratch/psych-src/matlab/R2014b MCRv84",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html#checking-available-versions",
    "href": "docs/software/freesurfer.html#checking-available-versions",
    "title": "FreeSurfer",
    "section": "",
    "text": "module avail freesurfer",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html#using-freesurfer-from-escience",
    "href": "docs/software/freesurfer.html#using-freesurfer-from-escience",
    "title": "FreeSurfer",
    "section": "",
    "text": "Load the default version with module load escience/freesurfer,\nor load a specific version with module load escience/freesurfer/&lt;version&gt;.\nAfter loading the module, FreeSurfer should have its environment set up for normal use.",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html#installing-a-different-version",
    "href": "docs/software/freesurfer.html#installing-a-different-version",
    "title": "FreeSurfer",
    "section": "",
    "text": "Note\n\n\n\nThis is tested from a VNC Apptainer environment with sufficient dependencies.\n\n\n\n\nGo to https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall and download the latest FreeSurfer tar archive for CentOS 8\nExtract the .tar.gz archive to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/.\n\nFor example, if installing version 7.3.2 of FreeSurfer, then run the following:\nLABNAME=escience\nVERSION=7.3.2\nmkdir -p \"/sw/contrib/$LABNAME-src/freesurfer/$VERSION\"\ntar -xzf freesurfer-linux-centos8_x86_64-$VERSION.tar.gz \\\n    --strip-components=1 \\\n    -C \"/sw/contrib/$LABNAME-src/freesurfer/$VERSION\"\n\nCreate an Lmod .lua module file for the new release with a text editor:\n\n\n\n/sw/contrib/modulefiles/mylabname/freesurfer/7.3.2.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\n--FreeSurfer 7.x requires MATLAB R2014b\ndepends_on(\"forsyth/matlab/r2014b\")\n\nsetenv(\"FREESURFER_HOME\", base)\nprepend_path(\"PATH\", pathJoin(base, \"bin\"))\nsource_sh(\"bash\", pathJoin(base, \"SetUpFreeSurfer.sh\"))\n\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/freesurfer\nmodule load mylabname/freesurfer/7.3.2\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.\n\n\nRequest a free license from https://surfer.nmr.mgh.harvard.edu/registration.html if not already.\n\n\n\n\n\n\n\nNote\n\n\n\nThis license is not tied to a specific version of FreeSurfer and can be copied from a previous installation.\n\n\n\nDownload license.txt from the registration email and copy it to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/license.txt.\nFor FS-FAST support, create a link to a supported MATLAB release (R2014b for FreeSurfer 7.x):\n\nmodule -I load labname/freesurfer/&lt;version&gt;\ncd \"$FREESURFER_HOME\"\nln -s /gscratch/psych-src/matlab/R2014b MCRv84",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/utilities/gdu.html",
    "href": "docs/software/utilities/gdu.html",
    "title": "Gdu",
    "section": "",
    "text": "Gdu is a disk usage analyzer that can be used to find out what is using up disk space. It is particularly useful for finding the largest files in your home directory or gscratch directories.\nGdu available as an Lmod module on Klone compute nodes.\n\n\nLike all Lmod modules, the gdu module is available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the module for CONN is installed in the escience hierarchy, its name is prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/gdu\nThe output should look a bit like this:\n---------------------------------------\n  escience/gdu: escience/gdu/5.25.0                                                           \n---------------------------------------    \n                                                                                              \n    This module can be loaded directly: module load escience/gdu/5.25.0                       \n                                                                                              \n    Help:                                                                                     \n      escience/gdu                                                                            \n\n\n\nLoad the default version with module load escience/gdu or load a specific version with module load escience/gdu/version.\nAfter loading the module, enter gdu into the shell to run the program.\n\n\n\nThe following demo shows how to use gdu to find the largest files and directories in the /sw/contrib/escience-src directory, where the contents of Lmod modules are stored.",
    "crumbs": [
      "Software",
      "Utilities",
      "Gdu"
    ]
  },
  {
    "objectID": "docs/software/utilities/gdu.html#use-with-lmod",
    "href": "docs/software/utilities/gdu.html#use-with-lmod",
    "title": "Gdu",
    "section": "",
    "text": "Like all Lmod modules, the gdu module is available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the module for CONN is installed in the escience hierarchy, its name is prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/gdu\nThe output should look a bit like this:\n---------------------------------------\n  escience/gdu: escience/gdu/5.25.0                                                           \n---------------------------------------    \n                                                                                              \n    This module can be loaded directly: module load escience/gdu/5.25.0                       \n                                                                                              \n    Help:                                                                                     \n      escience/gdu                                                                            \n\n\n\nLoad the default version with module load escience/gdu or load a specific version with module load escience/gdu/version.\nAfter loading the module, enter gdu into the shell to run the program.\n\n\n\nThe following demo shows how to use gdu to find the largest files and directories in the /sw/contrib/escience-src directory, where the contents of Lmod modules are stored.",
    "crumbs": [
      "Software",
      "Utilities",
      "Gdu"
    ]
  },
  {
    "objectID": "docs/storage/index.html",
    "href": "docs/storage/index.html",
    "title": "Storage",
    "section": "",
    "text": "Storage\nThis section describes the storage options available for your research data and provides information on concepts and terminology that will help you understand these options.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcepts and terminology\n\n\n\n\n\nConcepts, terminology, and considerations for storing research data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage on Klone\n\n\n\n\n\nOptions and best practices for managing storage on Klone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage options\n\n\n\n\n\nDifferent storage options for your research data\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Storage"
    ]
  },
  {
    "objectID": "docs/storage/options.html",
    "href": "docs/storage/options.html",
    "title": "Storage options",
    "section": "",
    "text": "There are many different storage options available for storing research data. This section describes some of the options available at the University of Washington, as well as some other options that may be useful.\n\n\nThe following table compares the different options for storing research data:\n\n\n\nFeature\nUW Psych Dept\nU Drive\nUW LOLO\nUW SFS\nUW OneDrive\nUW Google Drive\nRedCap\nResearchWorks\nAmazon S3\nAzure Blobs\nDropbox\nOSF\nGitHub\n\n\n\n\nSuitability\nGeneral use at UW Psych\nInteractive use at UW\nLong-term storage, archival\nInteractive use and collaboration at UW\nInteractive use and collaboration at UW\nInteractive use and collaboration at UW\nResearch data\nResearch data publication at UW\nProgramming, sharing, archival\nProgramming, sharing, archival\nInteractive use and collaboration\nResearch data, collaboration, publishing\nCode, software containers, collaboration\n\n\nUW support\n\n\n\n\n\n\n\n\nüí≤discount 5%\nüí≤discount ~10%\n\n\n\n\n\nStorage cost\nFree up to 2 TiB; 10 TiB free on request; more available for purchase (no recurring costs)\nFree up to 50 GiB, then $123/TiB/month ($0.12/GiB)\n$3.45/TiB/month\n$256/TiB/month\nFree, 5 TiB limit\nFree, 100 GiB limit\n\n\n~$25/month for 1 TiB hot data, ~$5/month for cold (S3 Glacier Instant Retrival)\n~$25/month for 1 TiB hot data, ~$5/month for cold; ~$30/TiB for cold egress\nSubscription ($12/month for 2 TiB)\nFree\nFree for most uses\n\n\nOff-campus availability\nVPN\nVPN\nVPN\nVPN\n\n\n\n\n\n\n\n\n\n\n\nAutomatic backups\n\n\n\n\nLimited\nLimited\n\n\n\n\n\n\n\n\n\nVersioning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlatforms\n  \n  \n  \n  \n   \n   \n\n\n   \n   \n    \n\n    \n\n\nEncryption in transit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncryption at rest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFERPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHIPAA\n\n\n\n\n\n\n\n\nOn request\n\n\n\n\n\n\nLocal filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSharing\n\n\nUW only\nUW only\nUW only\n\nUW only\n\n\n\n\n\n\n\n\nAvailable on Klone\nw/rclone\nw/rclone\n\n\nw/rclone\nw/rclone\n\n\nw/rclone\nw/rclone\nw/rclone\n\n\n\n\n\n\n\n\nUW provides several different storage options for faculty, staff, and students. Additionally, there are storage options that are not directly supported by UW but are available to UW users through UW‚Äôs enterprise agreements with the service providers, sometimes at a discount.\nThe following sections describe some of the most common options. UW IT maintains a page that provides an overview of UW‚Äôs online storage options, as well as a comparison of file service options. The UW IT Service Catalog has a more complete list of UW IT services.\nThe UW Libraries Research Data Services also maintains a page with information about storage options.\n\n\nThe Department of Psychology has a server that can be used for storing research data. This server is located in Guthrie Hall and is managed by the department‚Äôs IT staff. The server is not intended for sharing data with collaborators outside of the University of Washington. Contact the department‚Äôs IT staff for more information.\n\n\n\n2 TiB of storage (up to 10 TiB available on request; for more, contact mailto:dougkalk@uw.edu) per lab\n1 Gigabit Ethernet connection to campus network\nAccessible on Klone via rclone‚Äôs smb backend\nAccessible off-campus via VPN\nWeekly backups (twice a week on request) to a backup server\nMonthly backups to network-attached storage (NAS)\nStorage is not encrypted\nShared access typically by sharing a single username and password per lab\nAlternative configurations are possible on request (e.g., read-only accounts, restricted access to specific directories)\nMountable as a drive on Windows, Mac, and Linux systems\n\nThe department‚Äôs IT staff can provide more information about the server‚Äôs configuration and options. Contact mailto:dougkalk@uw.edu for more information.\n\n\n\n\n\n\nNote\n\n\n\nTransfer speed to the server is limited by the speed of the network connection and its disk drives. The server is connected to the campus network via a 1 Gigabit Ethernet connection. The maximum write speed is around 70-100 MiB/s.\n\n\n\n\n\nFree for departmental labs up to 10 TiB. More may be available on request.\n\n\n\n\nDepartment faculty and staff\n\n\n\n\n\nThe UW LOLO Data Archive provides long-term tape-backed archival storage for users at the university. It is only intended for use by UW faculty, staff, and affiliated organizations. It is suitable for data that is not actively being worked on. The Hyak documentation has additional information about the LOLO Data Archive.\n\n\n\n\nAll files immediately and directly accessible via SSH protocol\nSupport for up to 1,000 files per TB of data stored\nFast uploads and downloads for large (&gt;=100GB) files\n10Gbs network connection to campus, the internet, and Hyak\nTwo copies of all files are preserved, each in a separate data center, each in a different seismic zone\n\n\n\n\n\nThe LOLO Data Archive is priced at $3.45/TiB/month, with a minimum purchase of 1 TiB. The LOLO Data Archive is billed to a UW Workday worktag.\n\n\n\n\nUW faculty, staff, and affiliated organizations\nUW Workday worktag required\n\n\n\n\n\nThe UW Shared File Service provides a network file system that can be mounted on Linux and Windows systems. It is only intended for use by UW faculty, staff, and students. It is suitable for data that is actively being worked on.\n\n\n\n\nCIFS/SMB access from UW campus subnets. (Can also be accessed via VPN Service, which provides remote systems with a UW campus IP address.)\nSFTP access from any internet location.\nUser self-service file restores via ‚Äúsnapshots‚Äù\nDisaster recovery backups in two geographically separate regions (Seattle and Spokane). Per-file restores from tape are not available.\nEasy allocations of additional space as demand requires.\nAccess controlled via UW NetIDs and UW Groups\nPermissions are limited to Read/Write for a single group per folder, full granular ACLs are not supported currently.\n\n\n\n\n\n$0.25/GiB/month\n\n\n\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW Medical Center; Harborview Medical Center; requires a valid UW budget number\n\n\n\n\n\nThis service is considered FERPA compliant by the UW Registrar‚Äôs office (due to the process used for data release), but has had no other formal third party data security or privacy compliance audits.\n\n\n\n\n\nThe UW U Drive provides a network file system that can be mounted as a drive on Windows, Mac, and Linux 0systems on campus or by VPN.\n\n\n\n\nCIFS/SMB access from UW campus subnets. (Can also be accessed via VPN Service, which provides remote systems with a UW campus IP address.)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSFTP access to U Drive will no longer be available after March 20, 2024.\n\n\n\n\n\nFree up to 50 GiB. $0.12/GiB/month for additional storage.\n\n\n\n\nUW faculty; UW staff; UW students; UW students in residence halls; UW researchers\n\n\n\n\n\nUW provides access to the Microsoft OneDrive for Business cloud file syncing service as part of its Office 365 subscription.\n\n\n\n5 TiB of storage\n250 GiB maximum file size\nHIPAA and FERPA compliance\nSharing with UW users\nDesktop, browser, and mobile access\nSharePoint Online integration\nAccessible on Klone via rclone\n\n\n\n\n\n\n\nWarning\n\n\n\nOneDrive has technical limitations that may cause problems when using it with some applications (e.g., FreeSurfer). Caution is advised when working with files stored in OneDrive. The support page has more information about OneDrive‚Äôs limitations.\n\n\n\n\n\nFree for eligible UW users.\n\n\n\n\nUW faculty; UW staff; UW students; UW researchers; UW clinicians; For Shared UW NetIDs and Sponsored & affiliate UW NetIDs, accounts can be provisioned by UW employees\n\n\n\n\n\nUW Google Drive provides Google Drive file storage and sync for users at UW (this is not the same as the Google Drive service provided by Google).\n\n\n\n100 GiB of storage\nFERPA compliance\nAccessible on Klone via rclone\n\n\n\n\n\n\n\nWarning\n\n\n\nUW Google Drive is not HIPAA compliant.\n\n\n\n\n\n\n\nResearchWorks Archive is the University of Washington‚Äôs digital repository (also known as ‚Äúinstitutional repository‚Äù) for disseminating scholarly work. More information about ResearchWorks can be found on the Scholarly Publishing Services page.\n\nLink: ResearchWorks Archive\n\n\n\nRedCap is a web-based application for building and managing online surveys and databases. According to the RedCap website:\n\nResearch Electronic Data Capture (REDCap) is a rapidly evolving web tool developed by researchers for researchers in the translational domain.\nREDCap features a high degree of customizability for your forms and advanced user right control. It also features free, unlimited survey functionality, a sophisticated export module with support for all the popular statistical programs, and supports HIPAA compliance.\n\n\n\n\n\n\nAmazon Web Services (AWS) is a cloud computing platform that provides a number of different storage services, including Amazon S3, Amazon EBS, Amazon EFS, and Amazon Glacier. Amazon S3 is comparable to Azure Blobs. Amazon EBS is comparable to Azure Files. Amazon EFS is comparable to Azure Files. Amazon Glacier is comparable to Azure Archive Storage.\n\n\n\n\nData Egress Waiver, which effectively eliminates the standard charges for moving data out of the AWS Service.\nHIPAA Eligible Account, requires special request and approval, and is subject to important operational considerations to meet compliance requirements of HIPAA, the UW BAA, and UW Medicine Compliance Policies.\nAccessible on Klone via rclone\n\n\n\n\n\nThe approximate cost of storing 1 TiB of data per month is: - ~$25 for ‚Äúhot‚Äù data accessed frequently (S3 Standard) - ~$5/month for ‚Äúcold‚Äù data accessed infrequently (S3 Glacier Instant Retrieval)\nThe Data Egress Waiver eliminates the standard charges for moving data out of the AWS Service.\nUW IT has a subscription service for AWS that provides a discount of 5% and a waiver for data egress charges. The subscription is covered by UW‚Äôs HIPPAA BAA and other enterprise contracts. A UW Workday worktag is required to use the service. For more information, see the UW IT AWS page.\nFor detailed pricing information, see the AWS pricing calculator.\n\n\n\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW affiliated organizations; UW Medical Center; Harborview Medical Center; Any group with an approved UW blanket PO. NOTE: UW students are not eligible for this service, except when working within the scope of UW employment, e.g., as an RA, TA or GSA. Known Prerequisites: This service requires a valid UW budget number.\n\n\n\n\n\nMicrosoft‚Äôs Azure cloud platform includes a number of storage options, including Azure Blobs and Azure Files.\nAzure Blobs is comparable to Amazon AWS S3. It is designed to store large amounts of unstructured data but is not designed to be used as a file system.\n\n\n\nCovered by UW‚Äôs HIPAA BAA and other enterprise contracts\nAccessible on Klone via rclone\n\n\n\n\nThe approximate cost of storing 1 TiB of data per month is: - ~$25 for ‚Äúhot‚Äù data accessed frequently (Azure Blobs Hot Tier) - ~$5/month for ‚Äúcold‚Äù data accessed infrequently (Azure Blobs Cold Tier)\nUW IT has a subscription service for Azure that provides a discount of roughly 10% on Azure usage charges and allows charges to be paid with a UW Workday worktag. A UW Workday worktag is required to use the service. For more information, see the UW IT Azure Subscription Service.\nFor detailed pricing information, see the Azure pricing calculator.\n\n\n\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW affiliated organizations; UW Medical Center; Harborview Medical Center; Any group with an approved UW blanket PO. NOTE: UW students are not eligible for this service, except when working within the scope of UW employment, e.g., as an RA, TA or GSA. Known Prerequisites: This service requires a valid UW budget number.\n\nUW-IT‚Äôs Azure Subscription service allows UW units that wish to create and manage their own Azure subscription to receive a discount (~10%) on Azure usage charges and to pay those with a UW Workday worktag.\n\n\n\n\n\n\n\n\nGoogle Cloud Storage is a cloud object storage service that is part of the Google Cloud Platform. It resembles the offerings by AWS and Azure. It is distinct from UW Google Drive and Google Drive. Google Cloud Storage can be accessed on Klone via rclone.\n\n\n\n\n\n\nTip\n\n\n\nGoogle Cloud also offers block storage, file storage, and archival storage services. See here for more information.\n\n\n\n\n\nOpen Science Framework (OSF) is a web-based platform for managing research projects. OSF can be used to store research data and their metadata. It is best suited for collaborating with other researchers and sharing research data for papers. Storage addons are available to connect Amazon S3, Dropbox, GitHub, OneDrive, and GitHub to OSF.\n\n\n\nDropbox is a subscription-based cloud file sync service. The University of Washington does not support Dropbox. Dropbox is not suited for storing sensitive data. However, Dropbox can be useful for storing non-sensitive data that is actively being worked on, and it provides convenient versioning and recovery options. Dropbox can be accessed on Klone via rclone.\n\n\n\nGitHub is a web-based hosting service for version control using Git. Files up to 100 MiB each can be stored in a GitHub repository. The total size of a repository should be under 5 GiB.\nGitHub repositories can be made publicly available or can be made private. Private repositories are only accessible to collaborators who have been granted access to the repository. Private repositories are only available to collaborators who have GitHub accounts. GitHub repositories can be accessed from the GitHub website or the GitHub Desktop application.\n\n\nGitHub Releases can be used to store files up to 2 GiB in size each.\n\n\n\nGitHub Packages can be used to create container that can be uploaded to the GitHub Container Registry. For public repositories, there are no charges nor storage limits. For private repositories, storage limits and charges apply. A GitHub account is required to access GitHub Packages.\nAlthough GitHub Packages is not designed to store research data, it can be used to store research data in the form of a container containing an archive of research data (similar to a ZIP file). The data cannot be viewed or modified while stored in GitHub Packages, but can be downloaded and extracted from the container. Any changes to the data require creating a new image and uploading it to GitHub Packages.\nPublic packages can be accessed by anyone and should not be used for sensitive data. However, it may be possible to encrypt data before storing it in the image or to deploy an encrypted container.\n\n\n\nGitHub has a Large File Storage (LFS) extension that can be used to store large files in a GitHub repository.\nAccording to GitHub‚Äôs documentation, the file size limits for GitHub repositories are:\n\n\n\nPlan\nMaximum file size\n\n\n\n\nGitHub Free\n2 GiB\n\n\nGitHub Pro\n2 GiB\n\n\nGitHub Team\n4 GiB\n\n\nGitHub Enterprise Cloud\n5 GiB",
    "crumbs": [
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/storage/options.html#options-comparison",
    "href": "docs/storage/options.html#options-comparison",
    "title": "Storage options",
    "section": "",
    "text": "The following table compares the different options for storing research data:\n\n\n\nFeature\nUW Psych Dept\nU Drive\nUW LOLO\nUW SFS\nUW OneDrive\nUW Google Drive\nRedCap\nResearchWorks\nAmazon S3\nAzure Blobs\nDropbox\nOSF\nGitHub\n\n\n\n\nSuitability\nGeneral use at UW Psych\nInteractive use at UW\nLong-term storage, archival\nInteractive use and collaboration at UW\nInteractive use and collaboration at UW\nInteractive use and collaboration at UW\nResearch data\nResearch data publication at UW\nProgramming, sharing, archival\nProgramming, sharing, archival\nInteractive use and collaboration\nResearch data, collaboration, publishing\nCode, software containers, collaboration\n\n\nUW support\n\n\n\n\n\n\n\n\nüí≤discount 5%\nüí≤discount ~10%\n\n\n\n\n\nStorage cost\nFree up to 2 TiB; 10 TiB free on request; more available for purchase (no recurring costs)\nFree up to 50 GiB, then $123/TiB/month ($0.12/GiB)\n$3.45/TiB/month\n$256/TiB/month\nFree, 5 TiB limit\nFree, 100 GiB limit\n\n\n~$25/month for 1 TiB hot data, ~$5/month for cold (S3 Glacier Instant Retrival)\n~$25/month for 1 TiB hot data, ~$5/month for cold; ~$30/TiB for cold egress\nSubscription ($12/month for 2 TiB)\nFree\nFree for most uses\n\n\nOff-campus availability\nVPN\nVPN\nVPN\nVPN\n\n\n\n\n\n\n\n\n\n\n\nAutomatic backups\n\n\n\n\nLimited\nLimited\n\n\n\n\n\n\n\n\n\nVersioning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlatforms\n  \n  \n  \n  \n   \n   \n\n\n   \n   \n    \n\n    \n\n\nEncryption in transit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncryption at rest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFERPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHIPAA\n\n\n\n\n\n\n\n\nOn request\n\n\n\n\n\n\nLocal filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSharing\n\n\nUW only\nUW only\nUW only\n\nUW only\n\n\n\n\n\n\n\n\nAvailable on Klone\nw/rclone\nw/rclone\n\n\nw/rclone\nw/rclone\n\n\nw/rclone\nw/rclone\nw/rclone",
    "crumbs": [
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/storage/options.html#uw-supported-options",
    "href": "docs/storage/options.html#uw-supported-options",
    "title": "Storage options",
    "section": "",
    "text": "UW provides several different storage options for faculty, staff, and students. Additionally, there are storage options that are not directly supported by UW but are available to UW users through UW‚Äôs enterprise agreements with the service providers, sometimes at a discount.\nThe following sections describe some of the most common options. UW IT maintains a page that provides an overview of UW‚Äôs online storage options, as well as a comparison of file service options. The UW IT Service Catalog has a more complete list of UW IT services.\nThe UW Libraries Research Data Services also maintains a page with information about storage options.\n\n\nThe Department of Psychology has a server that can be used for storing research data. This server is located in Guthrie Hall and is managed by the department‚Äôs IT staff. The server is not intended for sharing data with collaborators outside of the University of Washington. Contact the department‚Äôs IT staff for more information.\n\n\n\n2 TiB of storage (up to 10 TiB available on request; for more, contact mailto:dougkalk@uw.edu) per lab\n1 Gigabit Ethernet connection to campus network\nAccessible on Klone via rclone‚Äôs smb backend\nAccessible off-campus via VPN\nWeekly backups (twice a week on request) to a backup server\nMonthly backups to network-attached storage (NAS)\nStorage is not encrypted\nShared access typically by sharing a single username and password per lab\nAlternative configurations are possible on request (e.g., read-only accounts, restricted access to specific directories)\nMountable as a drive on Windows, Mac, and Linux systems\n\nThe department‚Äôs IT staff can provide more information about the server‚Äôs configuration and options. Contact mailto:dougkalk@uw.edu for more information.\n\n\n\n\n\n\nNote\n\n\n\nTransfer speed to the server is limited by the speed of the network connection and its disk drives. The server is connected to the campus network via a 1 Gigabit Ethernet connection. The maximum write speed is around 70-100 MiB/s.\n\n\n\n\n\nFree for departmental labs up to 10 TiB. More may be available on request.\n\n\n\n\nDepartment faculty and staff\n\n\n\n\n\nThe UW LOLO Data Archive provides long-term tape-backed archival storage for users at the university. It is only intended for use by UW faculty, staff, and affiliated organizations. It is suitable for data that is not actively being worked on. The Hyak documentation has additional information about the LOLO Data Archive.\n\n\n\n\nAll files immediately and directly accessible via SSH protocol\nSupport for up to 1,000 files per TB of data stored\nFast uploads and downloads for large (&gt;=100GB) files\n10Gbs network connection to campus, the internet, and Hyak\nTwo copies of all files are preserved, each in a separate data center, each in a different seismic zone\n\n\n\n\n\nThe LOLO Data Archive is priced at $3.45/TiB/month, with a minimum purchase of 1 TiB. The LOLO Data Archive is billed to a UW Workday worktag.\n\n\n\n\nUW faculty, staff, and affiliated organizations\nUW Workday worktag required\n\n\n\n\n\nThe UW Shared File Service provides a network file system that can be mounted on Linux and Windows systems. It is only intended for use by UW faculty, staff, and students. It is suitable for data that is actively being worked on.\n\n\n\n\nCIFS/SMB access from UW campus subnets. (Can also be accessed via VPN Service, which provides remote systems with a UW campus IP address.)\nSFTP access from any internet location.\nUser self-service file restores via ‚Äúsnapshots‚Äù\nDisaster recovery backups in two geographically separate regions (Seattle and Spokane). Per-file restores from tape are not available.\nEasy allocations of additional space as demand requires.\nAccess controlled via UW NetIDs and UW Groups\nPermissions are limited to Read/Write for a single group per folder, full granular ACLs are not supported currently.\n\n\n\n\n\n$0.25/GiB/month\n\n\n\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW Medical Center; Harborview Medical Center; requires a valid UW budget number\n\n\n\n\n\nThis service is considered FERPA compliant by the UW Registrar‚Äôs office (due to the process used for data release), but has had no other formal third party data security or privacy compliance audits.\n\n\n\n\n\nThe UW U Drive provides a network file system that can be mounted as a drive on Windows, Mac, and Linux 0systems on campus or by VPN.\n\n\n\n\nCIFS/SMB access from UW campus subnets. (Can also be accessed via VPN Service, which provides remote systems with a UW campus IP address.)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSFTP access to U Drive will no longer be available after March 20, 2024.\n\n\n\n\n\nFree up to 50 GiB. $0.12/GiB/month for additional storage.\n\n\n\n\nUW faculty; UW staff; UW students; UW students in residence halls; UW researchers\n\n\n\n\n\nUW provides access to the Microsoft OneDrive for Business cloud file syncing service as part of its Office 365 subscription.\n\n\n\n5 TiB of storage\n250 GiB maximum file size\nHIPAA and FERPA compliance\nSharing with UW users\nDesktop, browser, and mobile access\nSharePoint Online integration\nAccessible on Klone via rclone\n\n\n\n\n\n\n\nWarning\n\n\n\nOneDrive has technical limitations that may cause problems when using it with some applications (e.g., FreeSurfer). Caution is advised when working with files stored in OneDrive. The support page has more information about OneDrive‚Äôs limitations.\n\n\n\n\n\nFree for eligible UW users.\n\n\n\n\nUW faculty; UW staff; UW students; UW researchers; UW clinicians; For Shared UW NetIDs and Sponsored & affiliate UW NetIDs, accounts can be provisioned by UW employees\n\n\n\n\n\nUW Google Drive provides Google Drive file storage and sync for users at UW (this is not the same as the Google Drive service provided by Google).\n\n\n\n100 GiB of storage\nFERPA compliance\nAccessible on Klone via rclone\n\n\n\n\n\n\n\nWarning\n\n\n\nUW Google Drive is not HIPAA compliant.\n\n\n\n\n\n\n\nResearchWorks Archive is the University of Washington‚Äôs digital repository (also known as ‚Äúinstitutional repository‚Äù) for disseminating scholarly work. More information about ResearchWorks can be found on the Scholarly Publishing Services page.\n\nLink: ResearchWorks Archive\n\n\n\nRedCap is a web-based application for building and managing online surveys and databases. According to the RedCap website:\n\nResearch Electronic Data Capture (REDCap) is a rapidly evolving web tool developed by researchers for researchers in the translational domain.\nREDCap features a high degree of customizability for your forms and advanced user right control. It also features free, unlimited survey functionality, a sophisticated export module with support for all the popular statistical programs, and supports HIPAA compliance.\n\n\n\n\n\n\nAmazon Web Services (AWS) is a cloud computing platform that provides a number of different storage services, including Amazon S3, Amazon EBS, Amazon EFS, and Amazon Glacier. Amazon S3 is comparable to Azure Blobs. Amazon EBS is comparable to Azure Files. Amazon EFS is comparable to Azure Files. Amazon Glacier is comparable to Azure Archive Storage.\n\n\n\n\nData Egress Waiver, which effectively eliminates the standard charges for moving data out of the AWS Service.\nHIPAA Eligible Account, requires special request and approval, and is subject to important operational considerations to meet compliance requirements of HIPAA, the UW BAA, and UW Medicine Compliance Policies.\nAccessible on Klone via rclone\n\n\n\n\n\nThe approximate cost of storing 1 TiB of data per month is: - ~$25 for ‚Äúhot‚Äù data accessed frequently (S3 Standard) - ~$5/month for ‚Äúcold‚Äù data accessed infrequently (S3 Glacier Instant Retrieval)\nThe Data Egress Waiver eliminates the standard charges for moving data out of the AWS Service.\nUW IT has a subscription service for AWS that provides a discount of 5% and a waiver for data egress charges. The subscription is covered by UW‚Äôs HIPPAA BAA and other enterprise contracts. A UW Workday worktag is required to use the service. For more information, see the UW IT AWS page.\nFor detailed pricing information, see the AWS pricing calculator.\n\n\n\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW affiliated organizations; UW Medical Center; Harborview Medical Center; Any group with an approved UW blanket PO. NOTE: UW students are not eligible for this service, except when working within the scope of UW employment, e.g., as an RA, TA or GSA. Known Prerequisites: This service requires a valid UW budget number.\n\n\n\n\n\nMicrosoft‚Äôs Azure cloud platform includes a number of storage options, including Azure Blobs and Azure Files.\nAzure Blobs is comparable to Amazon AWS S3. It is designed to store large amounts of unstructured data but is not designed to be used as a file system.\n\n\n\nCovered by UW‚Äôs HIPAA BAA and other enterprise contracts\nAccessible on Klone via rclone\n\n\n\n\nThe approximate cost of storing 1 TiB of data per month is: - ~$25 for ‚Äúhot‚Äù data accessed frequently (Azure Blobs Hot Tier) - ~$5/month for ‚Äúcold‚Äù data accessed infrequently (Azure Blobs Cold Tier)\nUW IT has a subscription service for Azure that provides a discount of roughly 10% on Azure usage charges and allows charges to be paid with a UW Workday worktag. A UW Workday worktag is required to use the service. For more information, see the UW IT Azure Subscription Service.\nFor detailed pricing information, see the Azure pricing calculator.\n\n\n\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW affiliated organizations; UW Medical Center; Harborview Medical Center; Any group with an approved UW blanket PO. NOTE: UW students are not eligible for this service, except when working within the scope of UW employment, e.g., as an RA, TA or GSA. Known Prerequisites: This service requires a valid UW budget number.\n\nUW-IT‚Äôs Azure Subscription service allows UW units that wish to create and manage their own Azure subscription to receive a discount (~10%) on Azure usage charges and to pay those with a UW Workday worktag.",
    "crumbs": [
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/storage/options.html#other-options",
    "href": "docs/storage/options.html#other-options",
    "title": "Storage options",
    "section": "",
    "text": "Google Cloud Storage is a cloud object storage service that is part of the Google Cloud Platform. It resembles the offerings by AWS and Azure. It is distinct from UW Google Drive and Google Drive. Google Cloud Storage can be accessed on Klone via rclone.\n\n\n\n\n\n\nTip\n\n\n\nGoogle Cloud also offers block storage, file storage, and archival storage services. See here for more information.\n\n\n\n\n\nOpen Science Framework (OSF) is a web-based platform for managing research projects. OSF can be used to store research data and their metadata. It is best suited for collaborating with other researchers and sharing research data for papers. Storage addons are available to connect Amazon S3, Dropbox, GitHub, OneDrive, and GitHub to OSF.\n\n\n\nDropbox is a subscription-based cloud file sync service. The University of Washington does not support Dropbox. Dropbox is not suited for storing sensitive data. However, Dropbox can be useful for storing non-sensitive data that is actively being worked on, and it provides convenient versioning and recovery options. Dropbox can be accessed on Klone via rclone.\n\n\n\nGitHub is a web-based hosting service for version control using Git. Files up to 100 MiB each can be stored in a GitHub repository. The total size of a repository should be under 5 GiB.\nGitHub repositories can be made publicly available or can be made private. Private repositories are only accessible to collaborators who have been granted access to the repository. Private repositories are only available to collaborators who have GitHub accounts. GitHub repositories can be accessed from the GitHub website or the GitHub Desktop application.\n\n\nGitHub Releases can be used to store files up to 2 GiB in size each.\n\n\n\nGitHub Packages can be used to create container that can be uploaded to the GitHub Container Registry. For public repositories, there are no charges nor storage limits. For private repositories, storage limits and charges apply. A GitHub account is required to access GitHub Packages.\nAlthough GitHub Packages is not designed to store research data, it can be used to store research data in the form of a container containing an archive of research data (similar to a ZIP file). The data cannot be viewed or modified while stored in GitHub Packages, but can be downloaded and extracted from the container. Any changes to the data require creating a new image and uploading it to GitHub Packages.\nPublic packages can be accessed by anyone and should not be used for sensitive data. However, it may be possible to encrypt data before storing it in the image or to deploy an encrypted container.\n\n\n\nGitHub has a Large File Storage (LFS) extension that can be used to store large files in a GitHub repository.\nAccording to GitHub‚Äôs documentation, the file size limits for GitHub repositories are:\n\n\n\nPlan\nMaximum file size\n\n\n\n\nGitHub Free\n2 GiB\n\n\nGitHub Pro\n2 GiB\n\n\nGitHub Team\n4 GiB\n\n\nGitHub Enterprise Cloud\n5 GiB",
    "crumbs": [
      "Storage",
      "Storage options"
    ]
  },
  {
    "objectID": "docs/start/index.html",
    "href": "docs/start/index.html",
    "title": "Getting started",
    "section": "",
    "text": "Getting started\n\nGet access to Hyak:\n\nActive UW students can join UW Research Computing Club (RCC) to gain access to STF-funded nodes.\nPIs or labs should purchase dedicated storage and compute resources.\n\nLog in to a cluster via SSH:\n\nssh your-uw-netid@klone.hyak.uw.edu\n\nNavigate and understand its compute infrastructure:\n\nRequest compute jobs with Slurm\nAccess certain programs with Lmod software modules\nBuild and run software containers with Apptainer\nCreate and access a virtual desktop with hyakvnc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to UW Hyak\n\n\n\n\n\nAn introduction to using the University of Washington Hyak Cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnecting to Hyak with SSH\n\n\n\n\n\nHow to connect to Hyak\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhyakvnc\n\n\n\n\n\nA program to create and manage VNC sessions running within a containerized Apptainer environment on a compute node\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html",
    "href": "docs/start/hyakvnc.html",
    "title": "hyakvnc",
    "section": "",
    "text": "hyakvnc is a program to create and manage VNC sessions running within an Apptainer environment on a compute node. # hyakvnc\nhyakvnc ‚Äì A tool for launching VNC sessions on Hyak.\nhyakvnc is a command-line tool that makes it easy to start a graphical VNC session on the University of Washington Hyak cluster, allowing you to interact with the system in a point-and-click environment, with support for graphical applications such as Freesurfer. hyakvnc sessions run in Apptainer containers, which provide reproducible environments that can run anywhere and be shared with other researchers.\nIf you‚Äôre already familiar with Hyak and VNC and you just want to install hyakvnc immediately, you can skip to the quick install section.\n\n\nBefore running hyakvnc, you‚Äôll need the following:\n\nA Linux, macOS, or Windows machine\nThe OpenSSH client (usually included with Linux and macOS, and available for Windows via WSL2 or Cygwin [note that the Windows 10+ built-in OpenSSH client will not work])\nA VNC client/viewer (TurboVNC viewer is recommended for all platforms)\nHyak Klone access with compute resources\nA private SSH key on your local machine which has been added to the authorized keys on the login node of the Hyak Klone cluster (see below)\nA HyakVNC-compatible Apptainer container image in a directory on Hyak (usually with the file extension .sif) or the URL to one (e.g,., oras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest)\n\nFollow the instructions below to set up your machine correctly:\n\n\n\n\nIf you are using Linux, OpenSSH is probably installed already ‚Äì if not, you can install it via apt-get install openssh-client on Debian/Ubuntu or yum install openssh-clients on RHEL/CentOS/Rocky/Fedora. To open a terminal window, search for ‚ÄúTerminal‚Äù in your desktop environment‚Äôs application launcher.\nTo install TurboVNC, download the latest version from here. On Debian/Ubuntu, you will need to download the file ending with arm64.deb. On RHEL/CentOS/Rocky/Fedora, you will need to download the file ending with x86_64.rpm. Then, install it by running sudo dpkg -i &lt;filename&gt; on Debian/Ubuntu or sudo rpm -i &lt;filename&gt; on RHEL/CentOS/Rocky/Fedora.\n\n\n\nIf you‚Äôre on macOS, OpenSSH will already be installed. To open a terminal window, open /Applications/Utilities/Terminal.app or search for ‚ÄúTerminal‚Äù in Launchpad or Spotlight.\nTo install TurboVNC, download the latest version from here. On an M1 Mac (newer), you will need to download the file ending with arm64.dmg. On an Intel Mac (older), you will need the file ending with x86_64.dmg. Then, open the .dmg file and launch the installer inside.\n\n\n\nWindows needs a little more setup. You‚Äôll need to install a terminal emulator as well as the OpenSSH client. The easiest way to do this is to install WSL2 (recommended for Windows versions 10+, comes with the OpenSSH client already installed) or Cygwin (not recommended, needs additional setup). See the links for instructions on how to install these. You can start a terminal window by searching for ‚ÄúTerminal‚Äù in the Start menu.\nTo install TurboVNC, download the latest version from here. You will need the file ending with x64.exe. Run the program to install TurboVNC.\n\n\n\n\nBefore you are allowed to connect to a compute node where your VNC session will be running, you must add your SSH public key to the authorized keys on the login node of the Hyak Klone cluster.\nIf you don‚Äôt, you will receive an error like this when you try to connect:\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic)\nTo set this up quickly on Linux, macOS, or Windows (WSL2/Cygwin), open a new terminal window on your machine and enter the following 2 commands before you try again. Replace your-uw-netid with your UW NetID:\n[ ! -r ~/.ssh/id_rsa ] && ssh-keygen -t rsa -b 4096 -N '' -C \"your-uw-netid@uw.edu\" -f ~/.ssh/id_rsa\nssh-copy-id -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa \"your-uw-netid\"@klone.hyak.uw.edu\nSee https://hyak.uw.edu/docs/setup/intracluster-keys for more information.\n\n\n\nYou‚Äôll need to find a HyakVNC-compatible container image to run your VNC session in. The following images are provided by us and can be used with hyakvnc by copying and pasting the URL into the hyakvnc create command:\n\noras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest ‚Äì Ubuntu 22.04 with TurboVNC\noras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-freesurfer-ubuntu22.04:latest ‚Äì Ubuntu 22.04 with TurboVNC and Freesurfer\n\n\n\n\n\nhyakvnc should be installed on the login node of the Hyak Klone cluster.\nTo connect to the login node, you‚Äôll need to enter the following command into a terminal window (replacing your-uw-netid with your UW NetID) and provide your password when prompted:\nssh your-uw-netid@klone.hyak.uw.edu\n\n\nAfter you‚Äôve connected to the login node, you can download and install hyakvnc by running the following command. Copy and paste it into the terminal window where you are connected to the login node and press enter:\neval \"$(curl -fsSL https://raw.githubusercontent.com/uw-psych/hyakvnc/main/install.sh)\"\nThis will download and install hyakvnc to your ~/.local/bin directory and add it to your $PATH so you can run it by typing hyakvnc into the terminal window.\n\n\n\nIn a terminal window connected to a login node, enter this command to clone the repository and navigate into the repository directory:\ngit clone --depth 1 --single-branch https://github.com/uw-psych/hyakvnc && cd hyakvnc\nThen, run the following command to install hyakvnc:\n./hyakvnc install\nIf you prefer, you may continue to use hyakvnc from the directory where you cloned it by running ./hyakvnc from that directory instead of using the command hyakvnc.\n\n\n\n\n\n\nStart a VNC session with the hyakvnc create command followed by arguments to specify the container. In this example, we‚Äôll use a basic container for a graphical environment from the HyakVNC GitHub Container Registry:\nhyakvnc create --container oras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest\nIt may take a few minutes to download the container if you‚Äôre running it the first time. If successful, hyakvnc should print commands and instructions to connect:\n==========\nCopy and paste these instructions into a command line terminal on your local machine to connect to the VNC session.\nYou may need to install a VNC client if you don't already have one.\n\nNOTE: If you receive an error that looks like \"Permission denied (publickey,gssapi-keyex,gssapi-with-mic)\", you don't have an SSH key set up.\nSee https://hyak.uw.edu/docs/setup/intracluster-keys for more information.\nTo set this up quickly on Linux, macOS, or Windows (WSL2/Cygwin), open a new terminal window on your machine and enter the following 2 commands before you try again:\n\n[ ! -r ~/.ssh/id_rsa ] && ssh-keygen -t rsa -b 4096 -N '' -C \"your-uw-netid@uw.edu\" -f ~/.ssh/id_rsa\nssh-copy-id -o StrictHostKeyChecking=no your-uw-netid@klone.hyak.uw.edu\n---------\nLINUX TERMINAL (bash/zsh):\nssh -f -o StrictHostKeyChecking=no -L 5901:/mmfs1/home/your-uw-netid/.hyakvnc/jobs/15042104/vnc/socket.uds -J your-uw-netid@klone.hyak.uw.edu your-uw-netid@g3053 sleep 20 && vncviewer localhost:5901 || xdg-open vnc://localhost:5901 || echo 'No VNC viewer found. Please install one or try entering the connection information manually.'\n\nMACOS TERMINAL\nssh -f -o StrictHostKeyChecking=no -L 5901:/mmfs1/home/your-uw-netid/.hyakvnc/jobs/15042104/vnc/socket.uds -J your-uw-netid@klone.hyak.uw.edu your-uw-netid@g3053 sleep 20 && open -b com.turbovnc.vncviewer.VncViewer --args localhost:5901 2&gt;/dev/null || open -b com.realvnc.vncviewer --args localhost:5901 2&gt;/dev/null || open -b com.tigervnc.vncviewer --args localhost:5901 2&gt;/dev/null || open vnc://localhost:5901 2&gt;/dev/null || echo 'No VNC viewer found. Please install one or try entering the connection information manually.'\n\nWINDOWS\nssh -f -o StrictHostKeyChecking=no -L 5901:/mmfs1/home/your-uw-netid/.hyakvnc/jobs/15042104/vnc/socket.uds -J your-uw-netid@klone.hyak.uw.edu your-uw-netid@g3053 sleep 20 && cmd.exe /c cmd /c \"$(cmd.exe /c where \"C:\\Program Files\\TurboVNC;C:\\Program Files(x86)\\TurboVNC:vncviewerw.bat\")\" localhost:5901 || echo 'No VNC viewer found. Please install one or try entering the connection information manually.'\n\n==========\n\n\n\n\nhyakvnc is command-line tool that only works on the login node of the Hyak cluster.\n\n\nUsage: hyakvnc create [create options...] -c &lt;container&gt; [extra args to pass to apptainer...]\n\nDescription:\n    Create a VNC session on Hyak.\n\nOptions:\n    -h, --help  Show this help message and exit\n    -c, --container Path to container image (required)\n    -A, --account   Slurm account to use (default: )\n    -p, --partition Slurm partition to use (default: )\n    -C, --cpus  Number of CPUs to request (default: 4)\n    -m, --mem   Amount of memory to request (default: 4G)\n    -t, --timelimit Slurm timelimit to use (default: 12:00:00)\n    -g, --gpus  Number of GPUs to request (default: )\n\nAdvanced options:\n    --no-ghcr-oras-preload  Don't preload ORAS GitHub Container Registry images\n\nExtra arguments:\n    Any extra arguments will be passed to apptainer run.\n    See 'apptainer run --help' for more information.\n\nExamples:\n    # Create a VNC session using the container ~/containers/mycontainer.sif\n    hyakvnc create -c ~/containers/mycontainer.sif\n    # Create a VNC session using the URL for a container:\n    hyakvnc create -c oras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest\n    # Use the SLURM account psych, the partition cpu-g2-mem2x, 4 CPUs, 1GB of memory, and 1 hour of time:\n    hyakvnc create -c ~/containers/mycontainer.sif -A psych -p cpu-g2-mem2x -C 4 -m 1G -t 1:00:00\n\n\n\n\nUsage: hyakvnc status [status options...]\n\nDescription:\n    Check status of VNC session(s) on Hyak.\n\nOptions:\n    -h, --help  Show this help message and exit\n    -d, --debug Print debug info\n    -j, --jobid Only check status of provided SLURM job ID (optional)\n\nExamples:\n    # Check the status of job no. 12345:\n    hyakvnc status -j 12345\n    # Check the status of all VNC jobs:\n    hyakvnc status\n\n\n\nUsage: hyakvnc show &lt;jobid&gt;\n    \nDescription:\n    Show connection information for a HyakVNC sesssion.\n    If no job ID is provided, a menu will be shown to select from running jobs.\n    \nOptions:\n    -h, --help  Show this help message and exit\n\nExamples:\n    # Show connection information for session running on job 123456:\n    hyakvnc show 123456\n    # Interactively select a job to show connection information for:\n    hyakvnc show\n\n    # Show connection information for session running on job 123456 for macOS:\n    hyakvnc show -s mac 123456\n\n\n\nUsage: hyakvnc stop [-a] [&lt;jobids&gt;...]\n    \nDescription:\n    Stop a provided HyakVNC sesssion and clean up its job directory.\n    If no job ID is provided, a menu will be shown to select from running jobs.\n\nOptions:\n    -h, --help  Show this help message and exit\n    -n, --no-cancel Don't cancel the SLURM job\n    -a, --all   Stop all jobs\n\nExamples:\n    # Stop a VNC session running on job 123456:\n    hyakvnc stop 123456\n    # Stop a VNC session running on job 123456 and do not cancel the job:\n    hyakvnc stop --no-cancel 123456\n    # Stop all VNC sessions:\n    hyakvnc stop -a\n    # Stop all VNC sessions but do not cancel the jobs:\n    hyakvnc stop -a -n\n\n\n\nUsage: hyakvnc config [config options...]\n    \nDescription:\n    Show the current configuration for hyakvnc, as set in the user configuration file at /home/runner/.hyakvnc/hyakvnc-config.env, in the current environment, or the default values set by hyakvnc.\n\nOptions:\n    -h, --help      Show this help message and exit\n\nExamples:\n    # Show configuration\n    hyakvnc config\n\n\n\nUsage: hyakvnc update [update options...]\n    \nDescription:\n    Update hyakvnc.\n\nOptions:\n    -h, --help          Show this help message and exit\n\nExamples:\n    # Update hyakvnc\n    hyakvnc update\n\n\n\nUsage: hyakvnc install [install options...]\n    \nDescription:\n    Install hyakvnc so the \"hyakvnc\" command can be run from anywhere.\n\nOptions:\n    -h, --help          Show this help message and exit\n    -i, --install-dir       Directory to install hyakvnc to (default: ~/.local/bin)\n    -s, --shell [bash|zsh]  Shell to install hyakvnc for (default: $SHELL or bash)\n\nExamples:\n    # Install\n    hyakvnc install\n    # Install to ~/bin:\n    hyakvnc install -i ~/bin\n\n\n\n\nThe following environment variables can be used to override the default settings. Any arguments passed to hyakvnc create will override the environment variables.\nYou can modify the values of these variables by:\n\nSetting and exporting them in your shell session, e.g.¬†export HYAKVNC_SLURM_MEM='8G' (which will only affect the current shell session)\nSetting them in your shell‚Äôs configuration file, e.g.¬†~/.bashrc or ~/.zshrc (which will affect all shell sessions)\nSetting them by prefixing the hyakvnc command with the variable assignment, e.g.¬†HYAKVNC_SLURM_MEM='8G' hyakvnc create ... (which will only affect the current command)\nSetting them in the file ~/.hyakvnc/hyakvnc-config.env (which will affect all hyakvnc commands)\n\nWhen you set an environment variable, it is advisable to surround the value with single quotes (') to prevent your shell from interpreting special characters. There should be no spaces between the variable name, the equals sign, and the value.\nThe following variables are available:\n\nHYAKVNC_DIR: Local directory to store application data (default: $HOME/.hyakvnc)\nHYAKVNC_CONFIG_FILE: Configuration file to use (default: $HYAKVNC_DIR/hyakvnc-config.env)\nHYAKVNC_CHECK_UPDATE_FREQUENCY: How often to check for updates in [d]ays or [m]inutes (default: 0 for every time. Use 1d for daily, 10m for every 10 minutes, etc. -1 to disable.)\nHYAKVNC_LOG_FILE: Log file to use (default: $HYAKVNC_DIR/hyakvnc.log)\nHYAKVNC_LOG_LEVEL: Log level to use for interactive output (default: INFO)\nHYAKVNC_LOG_FILE_LEVEL: Log level to use for log file output (default: DEBUG)\nHYAKVNC_SSH_HOST: Default SSH host to use for connection strings (default: klone.hyak.uw.edu)\nHYAKVNC_DEFAULT_TIMEOUT: Seconds to wait for most commands to complete before timing out (default: 30)\nHYAKVNC_VNC_PASSWORD: Password to use for new VNC sessions (default: password)\nHYAKVNC_VNC_DISPLAY: VNC display to use (default: :1)\nHYAKVNC_APPTAINER_CONTAINERS_DIR: Directory to look for apptainer containers (default: (none))\nHYAKVNC_APPTAINER_GHCR_ORAS_PRELOAD: Whether to preload SIF files from the ORAS GitHub Container Registry (default: 0)\nHYAKVNC_APPTAINER_BIN: Name of apptainer binary (default: apptainer)\nHYAKVNC_APPTAINER_CONTAINER: Path to container image to use (default: (none; set by --container option))\nHYAKVNC_APPTAINER_APP_VNCSERVER: Name of app in the container that starts the VNC session (default: vncserver)\nHYAKVNC_APPTAINER_APP_VNCKILL: Name of app that cleanly stops the VNC session in the container (default: vnckill)\nHYAKVNC_APPTAINER_WRITABLE_TMPFS: Whether to use a writable tmpfs for the container (default: 1)\nHYAKVNC_APPTAINER_CLEANENV: Whether to use a clean environment for the container (default: 1)\nHYAKVNC_APPTAINER_ADD_BINDPATHS: Bind paths to add to the container (default: (none))\nHYAKVNC_APPTAINER_ADD_ENVVARS: Environment variables to add to before invoking apptainer (default: (none))\nHYAKVNC_APPTAINER_ADD_ARGS: Additional arguments to give apptainer (default: (none))\nHYAKVNC_SLURM_JOB_PREFIX: Prefix to use for hyakvnc SLURM job names (default: hyakvnc-)\nHYAKVNC_SLURM_SUBMIT_TIMEOUT: Seconds after submitting job to wait for the job to start before timing out (default: 120)\nHYAKVNC_SLURM_OUTPUT_DIR: Directory to store SLURM output files (default: $HYAKVNC_DIR/slurm-output)\nHYAKVNC_SLURM_OUTPUT: Where to send SLURM job output (default: $HYAKVNC_SLURM_OUTPUT_DIR/job-%j.out)\nHYAKVNC_SLURM_JOB_NAME: What to name the launched SLURM job (default: (set according to container name))\nHYAKVNC_SLURM_ACCOUNT: Slurm account to use (default: (autodetected))\nHYAKVNC_SLURM_PARTITION: Slurm partition to use (default: (autodetected))\nHYAKVNC_SLURM_CLUSTER: Slurm cluster to use (default: (autodetected))\nHYAKVNC_SLURM_GPUS: Number of GPUs to request (default: (none))\nHYAKVNC_SLURM_MEM: Amount of memory to request, in [M]egabytes or [G]igabytes (default: 4G)\nHYAKVNC_SLURM_CPUS: Number of CPUs to request (default: 4)\nHYAKVNC_SLURM_TIMELIMIT: Time limit for SLURM job (default: 12:00:00)",
    "crumbs": [
      "Getting started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#prerequisites",
    "href": "docs/start/hyakvnc.html#prerequisites",
    "title": "hyakvnc",
    "section": "",
    "text": "Before running hyakvnc, you‚Äôll need the following:\n\nA Linux, macOS, or Windows machine\nThe OpenSSH client (usually included with Linux and macOS, and available for Windows via WSL2 or Cygwin [note that the Windows 10+ built-in OpenSSH client will not work])\nA VNC client/viewer (TurboVNC viewer is recommended for all platforms)\nHyak Klone access with compute resources\nA private SSH key on your local machine which has been added to the authorized keys on the login node of the Hyak Klone cluster (see below)\nA HyakVNC-compatible Apptainer container image in a directory on Hyak (usually with the file extension .sif) or the URL to one (e.g,., oras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest)\n\nFollow the instructions below to set up your machine correctly:\n\n\n\n\nIf you are using Linux, OpenSSH is probably installed already ‚Äì if not, you can install it via apt-get install openssh-client on Debian/Ubuntu or yum install openssh-clients on RHEL/CentOS/Rocky/Fedora. To open a terminal window, search for ‚ÄúTerminal‚Äù in your desktop environment‚Äôs application launcher.\nTo install TurboVNC, download the latest version from here. On Debian/Ubuntu, you will need to download the file ending with arm64.deb. On RHEL/CentOS/Rocky/Fedora, you will need to download the file ending with x86_64.rpm. Then, install it by running sudo dpkg -i &lt;filename&gt; on Debian/Ubuntu or sudo rpm -i &lt;filename&gt; on RHEL/CentOS/Rocky/Fedora.\n\n\n\nIf you‚Äôre on macOS, OpenSSH will already be installed. To open a terminal window, open /Applications/Utilities/Terminal.app or search for ‚ÄúTerminal‚Äù in Launchpad or Spotlight.\nTo install TurboVNC, download the latest version from here. On an M1 Mac (newer), you will need to download the file ending with arm64.dmg. On an Intel Mac (older), you will need the file ending with x86_64.dmg. Then, open the .dmg file and launch the installer inside.\n\n\n\nWindows needs a little more setup. You‚Äôll need to install a terminal emulator as well as the OpenSSH client. The easiest way to do this is to install WSL2 (recommended for Windows versions 10+, comes with the OpenSSH client already installed) or Cygwin (not recommended, needs additional setup). See the links for instructions on how to install these. You can start a terminal window by searching for ‚ÄúTerminal‚Äù in the Start menu.\nTo install TurboVNC, download the latest version from here. You will need the file ending with x64.exe. Run the program to install TurboVNC.\n\n\n\n\nBefore you are allowed to connect to a compute node where your VNC session will be running, you must add your SSH public key to the authorized keys on the login node of the Hyak Klone cluster.\nIf you don‚Äôt, you will receive an error like this when you try to connect:\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic)\nTo set this up quickly on Linux, macOS, or Windows (WSL2/Cygwin), open a new terminal window on your machine and enter the following 2 commands before you try again. Replace your-uw-netid with your UW NetID:\n[ ! -r ~/.ssh/id_rsa ] && ssh-keygen -t rsa -b 4096 -N '' -C \"your-uw-netid@uw.edu\" -f ~/.ssh/id_rsa\nssh-copy-id -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa \"your-uw-netid\"@klone.hyak.uw.edu\nSee https://hyak.uw.edu/docs/setup/intracluster-keys for more information.\n\n\n\nYou‚Äôll need to find a HyakVNC-compatible container image to run your VNC session in. The following images are provided by us and can be used with hyakvnc by copying and pasting the URL into the hyakvnc create command:\n\noras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest ‚Äì Ubuntu 22.04 with TurboVNC\noras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-freesurfer-ubuntu22.04:latest ‚Äì Ubuntu 22.04 with TurboVNC and Freesurfer",
    "crumbs": [
      "Getting started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#installing-hyakvnc",
    "href": "docs/start/hyakvnc.html#installing-hyakvnc",
    "title": "hyakvnc",
    "section": "",
    "text": "hyakvnc should be installed on the login node of the Hyak Klone cluster.\nTo connect to the login node, you‚Äôll need to enter the following command into a terminal window (replacing your-uw-netid with your UW NetID) and provide your password when prompted:\nssh your-uw-netid@klone.hyak.uw.edu\n\n\nAfter you‚Äôve connected to the login node, you can download and install hyakvnc by running the following command. Copy and paste it into the terminal window where you are connected to the login node and press enter:\neval \"$(curl -fsSL https://raw.githubusercontent.com/uw-psych/hyakvnc/main/install.sh)\"\nThis will download and install hyakvnc to your ~/.local/bin directory and add it to your $PATH so you can run it by typing hyakvnc into the terminal window.\n\n\n\nIn a terminal window connected to a login node, enter this command to clone the repository and navigate into the repository directory:\ngit clone --depth 1 --single-branch https://github.com/uw-psych/hyakvnc && cd hyakvnc\nThen, run the following command to install hyakvnc:\n./hyakvnc install\nIf you prefer, you may continue to use hyakvnc from the directory where you cloned it by running ./hyakvnc from that directory instead of using the command hyakvnc.",
    "crumbs": [
      "Getting started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#getting-started",
    "href": "docs/start/hyakvnc.html#getting-started",
    "title": "hyakvnc",
    "section": "",
    "text": "Start a VNC session with the hyakvnc create command followed by arguments to specify the container. In this example, we‚Äôll use a basic container for a graphical environment from the HyakVNC GitHub Container Registry:\nhyakvnc create --container oras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest\nIt may take a few minutes to download the container if you‚Äôre running it the first time. If successful, hyakvnc should print commands and instructions to connect:\n==========\nCopy and paste these instructions into a command line terminal on your local machine to connect to the VNC session.\nYou may need to install a VNC client if you don't already have one.\n\nNOTE: If you receive an error that looks like \"Permission denied (publickey,gssapi-keyex,gssapi-with-mic)\", you don't have an SSH key set up.\nSee https://hyak.uw.edu/docs/setup/intracluster-keys for more information.\nTo set this up quickly on Linux, macOS, or Windows (WSL2/Cygwin), open a new terminal window on your machine and enter the following 2 commands before you try again:\n\n[ ! -r ~/.ssh/id_rsa ] && ssh-keygen -t rsa -b 4096 -N '' -C \"your-uw-netid@uw.edu\" -f ~/.ssh/id_rsa\nssh-copy-id -o StrictHostKeyChecking=no your-uw-netid@klone.hyak.uw.edu\n---------\nLINUX TERMINAL (bash/zsh):\nssh -f -o StrictHostKeyChecking=no -L 5901:/mmfs1/home/your-uw-netid/.hyakvnc/jobs/15042104/vnc/socket.uds -J your-uw-netid@klone.hyak.uw.edu your-uw-netid@g3053 sleep 20 && vncviewer localhost:5901 || xdg-open vnc://localhost:5901 || echo 'No VNC viewer found. Please install one or try entering the connection information manually.'\n\nMACOS TERMINAL\nssh -f -o StrictHostKeyChecking=no -L 5901:/mmfs1/home/your-uw-netid/.hyakvnc/jobs/15042104/vnc/socket.uds -J your-uw-netid@klone.hyak.uw.edu your-uw-netid@g3053 sleep 20 && open -b com.turbovnc.vncviewer.VncViewer --args localhost:5901 2&gt;/dev/null || open -b com.realvnc.vncviewer --args localhost:5901 2&gt;/dev/null || open -b com.tigervnc.vncviewer --args localhost:5901 2&gt;/dev/null || open vnc://localhost:5901 2&gt;/dev/null || echo 'No VNC viewer found. Please install one or try entering the connection information manually.'\n\nWINDOWS\nssh -f -o StrictHostKeyChecking=no -L 5901:/mmfs1/home/your-uw-netid/.hyakvnc/jobs/15042104/vnc/socket.uds -J your-uw-netid@klone.hyak.uw.edu your-uw-netid@g3053 sleep 20 && cmd.exe /c cmd /c \"$(cmd.exe /c where \"C:\\Program Files\\TurboVNC;C:\\Program Files(x86)\\TurboVNC:vncviewerw.bat\")\" localhost:5901 || echo 'No VNC viewer found. Please install one or try entering the connection information manually.'\n\n==========",
    "crumbs": [
      "Getting started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#usage",
    "href": "docs/start/hyakvnc.html#usage",
    "title": "hyakvnc",
    "section": "",
    "text": "hyakvnc is command-line tool that only works on the login node of the Hyak cluster.\n\n\nUsage: hyakvnc create [create options...] -c &lt;container&gt; [extra args to pass to apptainer...]\n\nDescription:\n    Create a VNC session on Hyak.\n\nOptions:\n    -h, --help  Show this help message and exit\n    -c, --container Path to container image (required)\n    -A, --account   Slurm account to use (default: )\n    -p, --partition Slurm partition to use (default: )\n    -C, --cpus  Number of CPUs to request (default: 4)\n    -m, --mem   Amount of memory to request (default: 4G)\n    -t, --timelimit Slurm timelimit to use (default: 12:00:00)\n    -g, --gpus  Number of GPUs to request (default: )\n\nAdvanced options:\n    --no-ghcr-oras-preload  Don't preload ORAS GitHub Container Registry images\n\nExtra arguments:\n    Any extra arguments will be passed to apptainer run.\n    See 'apptainer run --help' for more information.\n\nExamples:\n    # Create a VNC session using the container ~/containers/mycontainer.sif\n    hyakvnc create -c ~/containers/mycontainer.sif\n    # Create a VNC session using the URL for a container:\n    hyakvnc create -c oras://ghcr.io/maouw/hyakvnc_apptainer/hyakvnc-vncserver-ubuntu22.04:latest\n    # Use the SLURM account psych, the partition cpu-g2-mem2x, 4 CPUs, 1GB of memory, and 1 hour of time:\n    hyakvnc create -c ~/containers/mycontainer.sif -A psych -p cpu-g2-mem2x -C 4 -m 1G -t 1:00:00\n\n\n\n\nUsage: hyakvnc status [status options...]\n\nDescription:\n    Check status of VNC session(s) on Hyak.\n\nOptions:\n    -h, --help  Show this help message and exit\n    -d, --debug Print debug info\n    -j, --jobid Only check status of provided SLURM job ID (optional)\n\nExamples:\n    # Check the status of job no. 12345:\n    hyakvnc status -j 12345\n    # Check the status of all VNC jobs:\n    hyakvnc status\n\n\n\nUsage: hyakvnc show &lt;jobid&gt;\n    \nDescription:\n    Show connection information for a HyakVNC sesssion.\n    If no job ID is provided, a menu will be shown to select from running jobs.\n    \nOptions:\n    -h, --help  Show this help message and exit\n\nExamples:\n    # Show connection information for session running on job 123456:\n    hyakvnc show 123456\n    # Interactively select a job to show connection information for:\n    hyakvnc show\n\n    # Show connection information for session running on job 123456 for macOS:\n    hyakvnc show -s mac 123456\n\n\n\nUsage: hyakvnc stop [-a] [&lt;jobids&gt;...]\n    \nDescription:\n    Stop a provided HyakVNC sesssion and clean up its job directory.\n    If no job ID is provided, a menu will be shown to select from running jobs.\n\nOptions:\n    -h, --help  Show this help message and exit\n    -n, --no-cancel Don't cancel the SLURM job\n    -a, --all   Stop all jobs\n\nExamples:\n    # Stop a VNC session running on job 123456:\n    hyakvnc stop 123456\n    # Stop a VNC session running on job 123456 and do not cancel the job:\n    hyakvnc stop --no-cancel 123456\n    # Stop all VNC sessions:\n    hyakvnc stop -a\n    # Stop all VNC sessions but do not cancel the jobs:\n    hyakvnc stop -a -n\n\n\n\nUsage: hyakvnc config [config options...]\n    \nDescription:\n    Show the current configuration for hyakvnc, as set in the user configuration file at /home/runner/.hyakvnc/hyakvnc-config.env, in the current environment, or the default values set by hyakvnc.\n\nOptions:\n    -h, --help      Show this help message and exit\n\nExamples:\n    # Show configuration\n    hyakvnc config\n\n\n\nUsage: hyakvnc update [update options...]\n    \nDescription:\n    Update hyakvnc.\n\nOptions:\n    -h, --help          Show this help message and exit\n\nExamples:\n    # Update hyakvnc\n    hyakvnc update\n\n\n\nUsage: hyakvnc install [install options...]\n    \nDescription:\n    Install hyakvnc so the \"hyakvnc\" command can be run from anywhere.\n\nOptions:\n    -h, --help          Show this help message and exit\n    -i, --install-dir       Directory to install hyakvnc to (default: ~/.local/bin)\n    -s, --shell [bash|zsh]  Shell to install hyakvnc for (default: $SHELL or bash)\n\nExamples:\n    # Install\n    hyakvnc install\n    # Install to ~/bin:\n    hyakvnc install -i ~/bin",
    "crumbs": [
      "Getting started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#configuration",
    "href": "docs/start/hyakvnc.html#configuration",
    "title": "hyakvnc",
    "section": "",
    "text": "The following environment variables can be used to override the default settings. Any arguments passed to hyakvnc create will override the environment variables.\nYou can modify the values of these variables by:\n\nSetting and exporting them in your shell session, e.g.¬†export HYAKVNC_SLURM_MEM='8G' (which will only affect the current shell session)\nSetting them in your shell‚Äôs configuration file, e.g.¬†~/.bashrc or ~/.zshrc (which will affect all shell sessions)\nSetting them by prefixing the hyakvnc command with the variable assignment, e.g.¬†HYAKVNC_SLURM_MEM='8G' hyakvnc create ... (which will only affect the current command)\nSetting them in the file ~/.hyakvnc/hyakvnc-config.env (which will affect all hyakvnc commands)\n\nWhen you set an environment variable, it is advisable to surround the value with single quotes (') to prevent your shell from interpreting special characters. There should be no spaces between the variable name, the equals sign, and the value.\nThe following variables are available:\n\nHYAKVNC_DIR: Local directory to store application data (default: $HOME/.hyakvnc)\nHYAKVNC_CONFIG_FILE: Configuration file to use (default: $HYAKVNC_DIR/hyakvnc-config.env)\nHYAKVNC_CHECK_UPDATE_FREQUENCY: How often to check for updates in [d]ays or [m]inutes (default: 0 for every time. Use 1d for daily, 10m for every 10 minutes, etc. -1 to disable.)\nHYAKVNC_LOG_FILE: Log file to use (default: $HYAKVNC_DIR/hyakvnc.log)\nHYAKVNC_LOG_LEVEL: Log level to use for interactive output (default: INFO)\nHYAKVNC_LOG_FILE_LEVEL: Log level to use for log file output (default: DEBUG)\nHYAKVNC_SSH_HOST: Default SSH host to use for connection strings (default: klone.hyak.uw.edu)\nHYAKVNC_DEFAULT_TIMEOUT: Seconds to wait for most commands to complete before timing out (default: 30)\nHYAKVNC_VNC_PASSWORD: Password to use for new VNC sessions (default: password)\nHYAKVNC_VNC_DISPLAY: VNC display to use (default: :1)\nHYAKVNC_APPTAINER_CONTAINERS_DIR: Directory to look for apptainer containers (default: (none))\nHYAKVNC_APPTAINER_GHCR_ORAS_PRELOAD: Whether to preload SIF files from the ORAS GitHub Container Registry (default: 0)\nHYAKVNC_APPTAINER_BIN: Name of apptainer binary (default: apptainer)\nHYAKVNC_APPTAINER_CONTAINER: Path to container image to use (default: (none; set by --container option))\nHYAKVNC_APPTAINER_APP_VNCSERVER: Name of app in the container that starts the VNC session (default: vncserver)\nHYAKVNC_APPTAINER_APP_VNCKILL: Name of app that cleanly stops the VNC session in the container (default: vnckill)\nHYAKVNC_APPTAINER_WRITABLE_TMPFS: Whether to use a writable tmpfs for the container (default: 1)\nHYAKVNC_APPTAINER_CLEANENV: Whether to use a clean environment for the container (default: 1)\nHYAKVNC_APPTAINER_ADD_BINDPATHS: Bind paths to add to the container (default: (none))\nHYAKVNC_APPTAINER_ADD_ENVVARS: Environment variables to add to before invoking apptainer (default: (none))\nHYAKVNC_APPTAINER_ADD_ARGS: Additional arguments to give apptainer (default: (none))\nHYAKVNC_SLURM_JOB_PREFIX: Prefix to use for hyakvnc SLURM job names (default: hyakvnc-)\nHYAKVNC_SLURM_SUBMIT_TIMEOUT: Seconds after submitting job to wait for the job to start before timing out (default: 120)\nHYAKVNC_SLURM_OUTPUT_DIR: Directory to store SLURM output files (default: $HYAKVNC_DIR/slurm-output)\nHYAKVNC_SLURM_OUTPUT: Where to send SLURM job output (default: $HYAKVNC_SLURM_OUTPUT_DIR/job-%j.out)\nHYAKVNC_SLURM_JOB_NAME: What to name the launched SLURM job (default: (set according to container name))\nHYAKVNC_SLURM_ACCOUNT: Slurm account to use (default: (autodetected))\nHYAKVNC_SLURM_PARTITION: Slurm partition to use (default: (autodetected))\nHYAKVNC_SLURM_CLUSTER: Slurm cluster to use (default: (autodetected))\nHYAKVNC_SLURM_GPUS: Number of GPUs to request (default: (none))\nHYAKVNC_SLURM_MEM: Amount of memory to request, in [M]egabytes or [G]igabytes (default: 4G)\nHYAKVNC_SLURM_CPUS: Number of CPUs to request (default: 4)\nHYAKVNC_SLURM_TIMELIMIT: Time limit for SLURM job (default: 12:00:00)",
    "crumbs": [
      "Getting started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nWelcome to the documentation for computing resources at the University of Washington Psychology Department. This documentation is intended to be a living document and we welcome contributions from the community.\nHere are some quick links to get you started:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\nGetting started with the Hyak cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\n\n\n\nUsing and installing software on Klone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage\n\n\n\n\n\nStoring research data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the cluster\n\n\n\n\n\nHow to connect to and use the UW Hyak cluster\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "docs/start/introduction.html",
    "href": "docs/start/introduction.html",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "Hyak is the University of Washington‚Äôs high-performance computing cluster:\n\nHyak is part of the University of Washington‚Äôs cyberinfrastructure plan to support world-class research in every department. Hyak is an ecosystem of high-performance compute (HPC) clusters and supporting infrastructure (e.g., data management, computational training, scientific consulting). We are currently on the 3rd generation cluster for Hyak.\n\n\n\nThe latest generation of the Hyak cluster is Klone.\n\n\n\nThe second generation of the Hyak cluster, Mox, is being phased out in 2024. No new capacity or node additions are planned. The Klone cluster is recommended for all users.\n\n\n\nSign up to the psych-hpc mailing list to ask questions and receive updates about computing at the UW Psychology Department.\nCheck out UW Research Computing Club‚Äôs Slack and ask questions under hyak-questions.\nFor more information, see the official Hyak website.",
    "crumbs": [
      "Getting started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/introduction.html#klone",
    "href": "docs/start/introduction.html#klone",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "The latest generation of the Hyak cluster is Klone.",
    "crumbs": [
      "Getting started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/introduction.html#mox",
    "href": "docs/start/introduction.html#mox",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "The second generation of the Hyak cluster, Mox, is being phased out in 2024. No new capacity or node additions are planned. The Klone cluster is recommended for all users.",
    "crumbs": [
      "Getting started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/introduction.html#resources",
    "href": "docs/start/introduction.html#resources",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "Sign up to the psych-hpc mailing list to ask questions and receive updates about computing at the UW Psychology Department.\nCheck out UW Research Computing Club‚Äôs Slack and ask questions under hyak-questions.\nFor more information, see the official Hyak website.",
    "crumbs": [
      "Getting started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html",
    "href": "docs/start/connect-ssh.html",
    "title": "Connecting to Hyak with SSH",
    "section": "",
    "text": "SSH (secure shell) is the primary method for connecting to and interacting with UW Hyak clusters from a command-line interface (CLI).\n\n\n\n\n\n\nImportant\n\n\n\nConnections to UW Hyak are authenticated with your UW NetID credentials and Duo for two-factor. Alternative login methods, including SSH key authentication, cannot be used to log in.\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse an SSH client that supports session multiplexing/sharing to reuse an active session without two-factor authentication.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nmacOS, most Linux distributions, and (newer builds of) Windows 10/11 already include OpenSSH client by default.\n\n\nPick your platform and install an SSH client:\n\nLinuxmacOSWindows\n\n\nOpen a terminal and install OpenSSH client with your distribution‚Äôs package manager.\n\n\n\nDistribution\nCommand\n\n\n\n\nUbuntu/Debian/Linux Mint\nsudo apt-get install openssh\n\n\nRHEL/CentOS/Fedora/Rocky\nsudo dnf install openssh\n\n\nSUSE\nsudo zypper install openssh\n\n\nAlpine\napk add openssh\n\n\nArch\nsudo pacman -S openssh\n\n\n\nTo set up session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nmacOS provides OpenSSH by default. It is accessible from the command line with Terminal.app.\nTo set up session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nThere are many SSH clients available for the Windows platform. Here is a short table comparing features provided by each SSH client:\n\n\n\n\n\nSSH Client\nPort-Forwarding\nX11\nSession Sharing\nInterface\nFile Transfer Interface\n\n\n\n\nWin32-OpenSSH\nSupported\nRequires X11 Server[^x11]\nUnsupported\nCLI\nCLI\n\n\nMobaXterm\nSupported\nSupported\nUnsupported\nGUI/CLI\nGUI/CLI\n\n\nPuTTY\nSupported\nRequires X11 Server[^x11]\nSupported\nGUI\nCLI\n\n\nMSYS2+OpenSSH\nSupported\nRequires X11 Server[^x11]\nUnsupported\nCLI\nCLI\n\n\nWSL2+OpenSSH\nSupported\nSupported\nSupported\nCLI\nCLI\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPuTTY‚Äôs plink.exe tool is not suited for interactive sessions as it cannot handle many key inputs (arrow keys for cursor movement, backspace for character deletion).\n\n\n\n\n\n\n\n\nSession Sharing in PuTTY\n\n\n\n\n\nCheck Share SSH Connection if possible in the PuTTY Configuration window under Connection-&gt;SSH:\n\nSave a PuTTY profile under Session with the hostname set to UWNetID@klone.hyak.uw.edu:\n\nWhile PuTTY maintains an active session with session sharing enabled, all (GUI/CLI) PuTTY tools can reuse the active authenticated session as long as the terminal window remains open and active. If all sessions close, authentication will be required.\nTo create a new terminal window, right-click the title bar of an active terminal window, then click on Duplicate Session.\nTo reuse an active session with CLI tools, specify the name of the saved PuTTY profile in place of UWNetID@klone.hyak.uw.edu.\n\n\n\n\n\n\n\n\n\n\n\nOpenSSHPuTTYMobaXterm GUI\n\n\n\nOpen a terminal instance.\n\n\n\n\n\n\n\nNote\n\n\n\nWindows users should open the PowerShell console or install and use Windows Terminal from the Microsoft Store app.\n\n\n\nConnect to Hyak Klone cluster with ssh command with your UW NetID:\n\nssh UWNetID@klone.hyak.uw.edu\n\nIf prompted to ‚Äúcontinue connecting,‚Äù type yes and press enter.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen PuTTY:\n\n\n\nCheck Share SSH Connection if possible under Connection-&gt;SSH:\n\n\n\nUnder Session, set the hostname to UWNetID@klone.hyak.uw.edu and save the profile as Klone:\n\n\n\nPress Open at the bottom of the configuration window to start the connection.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen MobaXterm:\n\n\n\nClick the Session icon in the top left corner.\nSelect SSH under the Session settings window, then do the following:\n\n\nset the remote host to klone.hyak.uw.edu\ncheck Specify username and specify your UW NetID\n\n\n\nPress OK at the bottom of the window to start the connection.\nPress Accept if prompted to trust the identity of the remote host:\n\n\n\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\n\n\n\n\nOpenSSHPuTTY CLI\n\n\nscp is an OpenSSH client utility for copying files and directories to/from a remote target. The general syntax has the following form:\nscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\nscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nTip\n\n\n\nTo copy a file to Klone at some path, run:\nscp /path/to/my/file UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\n\n\n\n\npscp is a CLI utility (provided by a standard PuTTY installation) for copying files.\npscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\npscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nNote\n\n\n\npscp does not support the use of the tilde (~) as a shortcut to the home directory. By default, relative paths always start from home directory.\n# copy file to home directory\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:\n\n\n\n\n\n\n\n\nTip\n\n\n\npscp can reuse/share an active SSH connection without re-authorization if using a saved PuTTY profile with Share SSH Connection if possible enabled.\n\n\nTo send a file to Klone at some path, run:\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\nAlternatively, we can use the name of the saved PuTTY profile (Klone for this example) to reuse an active connection to copy a file to Klone:\npscp.exe \"c:\\path\\to\\my\\file\" Klone:/gscratch/mylab/\nTo copy a directory to Klone, use the -r argument to copy directories and files recursively:\npscp.exe -r c:\\path\\to\\my\\directory\\ Klone:/gscratch/mylab/",
    "crumbs": [
      "Getting started",
      "Connecting to Hyak with SSH"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html#installing-an-ssh-client",
    "href": "docs/start/connect-ssh.html#installing-an-ssh-client",
    "title": "Connecting to Hyak with SSH",
    "section": "",
    "text": "Note\n\n\n\nmacOS, most Linux distributions, and (newer builds of) Windows 10/11 already include OpenSSH client by default.\n\n\nPick your platform and install an SSH client:\n\nLinuxmacOSWindows\n\n\nOpen a terminal and install OpenSSH client with your distribution‚Äôs package manager.\n\n\n\nDistribution\nCommand\n\n\n\n\nUbuntu/Debian/Linux Mint\nsudo apt-get install openssh\n\n\nRHEL/CentOS/Fedora/Rocky\nsudo dnf install openssh\n\n\nSUSE\nsudo zypper install openssh\n\n\nAlpine\napk add openssh\n\n\nArch\nsudo pacman -S openssh\n\n\n\nTo set up session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nmacOS provides OpenSSH by default. It is accessible from the command line with Terminal.app.\nTo set up session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nThere are many SSH clients available for the Windows platform. Here is a short table comparing features provided by each SSH client:\n\n\n\n\n\nSSH Client\nPort-Forwarding\nX11\nSession Sharing\nInterface\nFile Transfer Interface\n\n\n\n\nWin32-OpenSSH\nSupported\nRequires X11 Server[^x11]\nUnsupported\nCLI\nCLI\n\n\nMobaXterm\nSupported\nSupported\nUnsupported\nGUI/CLI\nGUI/CLI\n\n\nPuTTY\nSupported\nRequires X11 Server[^x11]\nSupported\nGUI\nCLI\n\n\nMSYS2+OpenSSH\nSupported\nRequires X11 Server[^x11]\nUnsupported\nCLI\nCLI\n\n\nWSL2+OpenSSH\nSupported\nSupported\nSupported\nCLI\nCLI\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPuTTY‚Äôs plink.exe tool is not suited for interactive sessions as it cannot handle many key inputs (arrow keys for cursor movement, backspace for character deletion).\n\n\n\n\n\n\n\n\nSession Sharing in PuTTY\n\n\n\n\n\nCheck Share SSH Connection if possible in the PuTTY Configuration window under Connection-&gt;SSH:\n\nSave a PuTTY profile under Session with the hostname set to UWNetID@klone.hyak.uw.edu:\n\nWhile PuTTY maintains an active session with session sharing enabled, all (GUI/CLI) PuTTY tools can reuse the active authenticated session as long as the terminal window remains open and active. If all sessions close, authentication will be required.\nTo create a new terminal window, right-click the title bar of an active terminal window, then click on Duplicate Session.\nTo reuse an active session with CLI tools, specify the name of the saved PuTTY profile in place of UWNetID@klone.hyak.uw.edu.",
    "crumbs": [
      "Getting started",
      "Connecting to Hyak with SSH"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html#connecting-via-ssh",
    "href": "docs/start/connect-ssh.html#connecting-via-ssh",
    "title": "Connecting to Hyak with SSH",
    "section": "",
    "text": "OpenSSHPuTTYMobaXterm GUI\n\n\n\nOpen a terminal instance.\n\n\n\n\n\n\n\nNote\n\n\n\nWindows users should open the PowerShell console or install and use Windows Terminal from the Microsoft Store app.\n\n\n\nConnect to Hyak Klone cluster with ssh command with your UW NetID:\n\nssh UWNetID@klone.hyak.uw.edu\n\nIf prompted to ‚Äúcontinue connecting,‚Äù type yes and press enter.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen PuTTY:\n\n\n\nCheck Share SSH Connection if possible under Connection-&gt;SSH:\n\n\n\nUnder Session, set the hostname to UWNetID@klone.hyak.uw.edu and save the profile as Klone:\n\n\n\nPress Open at the bottom of the configuration window to start the connection.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen MobaXterm:\n\n\n\nClick the Session icon in the top left corner.\nSelect SSH under the Session settings window, then do the following:\n\n\nset the remote host to klone.hyak.uw.edu\ncheck Specify username and specify your UW NetID\n\n\n\nPress OK at the bottom of the window to start the connection.\nPress Accept if prompted to trust the identity of the remote host:\n\n\n\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.",
    "crumbs": [
      "Getting started",
      "Connecting to Hyak with SSH"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html#transferring-files-tofrom-hyak",
    "href": "docs/start/connect-ssh.html#transferring-files-tofrom-hyak",
    "title": "Connecting to Hyak with SSH",
    "section": "",
    "text": "OpenSSHPuTTY CLI\n\n\nscp is an OpenSSH client utility for copying files and directories to/from a remote target. The general syntax has the following form:\nscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\nscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nTip\n\n\n\nTo copy a file to Klone at some path, run:\nscp /path/to/my/file UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\n\n\n\n\npscp is a CLI utility (provided by a standard PuTTY installation) for copying files.\npscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\npscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nNote\n\n\n\npscp does not support the use of the tilde (~) as a shortcut to the home directory. By default, relative paths always start from home directory.\n# copy file to home directory\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:\n\n\n\n\n\n\n\n\nTip\n\n\n\npscp can reuse/share an active SSH connection without re-authorization if using a saved PuTTY profile with Share SSH Connection if possible enabled.\n\n\nTo send a file to Klone at some path, run:\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\nAlternatively, we can use the name of the saved PuTTY profile (Klone for this example) to reuse an active connection to copy a file to Klone:\npscp.exe \"c:\\path\\to\\my\\file\" Klone:/gscratch/mylab/\nTo copy a directory to Klone, use the -r argument to copy directories and files recursively:\npscp.exe -r c:\\path\\to\\my\\directory\\ Klone:/gscratch/mylab/",
    "crumbs": [
      "Getting started",
      "Connecting to Hyak with SSH"
    ]
  },
  {
    "objectID": "docs/storage/klone-storage.html",
    "href": "docs/storage/klone-storage.html",
    "title": "Storage on Klone",
    "section": "",
    "text": "Klone has a number of storage options available to users. This page describes the options and best practices for using them.\n\n\nThe Klone cluster makes available several locations for storing data, some of which are shared across the login nodes and compute nodes, and some of which are only available to the node where a job is running.\n\n\nThe following storage locations are shared across the login nodes and compute nodes:\n\n\nEvery user on Klone has a home directory, which is the default location for storing files. The home directory is located at /mmfs1/home/&lt;username&gt;. The home directory is where you are when you log in to the login node, and it is also acessible by any compute node where you have a job running.\n\nNone of the storage locations on Klone are backed up. If you need to store data that you cannot afford to lose, you should use a different storage location ‚Äì see our guide to storage for more information.\n\nCurrently, there is a 10 GiB quota on the home directory. This quota is enforced by the system, and you will not be able to write to your home directory if you exceed it. You can check your quota with the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --home\n\n1\n\nThe --home flag tells hyakstorage to check your home directory quota.\n\n\nThe result will look something like this:\n                       Usage report for /mmfs1/home/altan\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 1GB / 10GB                 ‚îÇ 11579 / 256000 files        ‚îÇ\n‚îÇ                   ‚îÇ 10%                        ‚îÇ 5%                          ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nIt is important to note that the home directory is not intended for storing large amounts of data. If you need to store more than a few GiB of data, you should use one of the other storage options described below.\n\n\n\n\n\n\nDefault software installation locations\n\n\n\nA lot of software will save files to your home directory by default. Python, for example, will install packages to ~/.local/lib/python3.6 by default when they are installed using pip install --user. Apptainer will also cache images in ~/.apptainer/cache by default. Fortunately, both of these can be changed by setting environment variables or using solutions like virtual environments.\nIf you are installing software, you should always check to see where it is being installed, and make sure that it is not being installed to your home directory unless you have a good reason for doing so.\n\n\n\n\n\nEvery group on Klone has a directory under /mmfs1/gscratch (also aliased as /gscratch), which is a good place to store data that you are using for a job, as well as any data and libraries you install. The gscratch directory is accessible from any node.\nEach group has a quota on their gscratch directory, and you will not be able to write to your group‚Äôs gscratch directory if you exceed it.\nYou can find out the locataion of your group‚Äôs gscratch directory and see how much is used of your available capacity using the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --gscratch\n\n1\n\nThe --gscratch flag tells hyakstorage to check your group‚Äôs gscratch quota.\n\n\nThe result will look something like this:\n                   Usage report for /mmfs1/gscratch/escience\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 3305GB / 4096GB            ‚îÇ 710698 / 4000000 files      ‚îÇ\n‚îÇ                   ‚îÇ 81%                        ‚îÇ 18%                         ‚îÇ\n‚îÇ My usage:         ‚îÇ 41GB                       ‚îÇ 11776 files                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nThe line at the top shows the location of your group‚Äôs gscratch directory.\nEvery lab or group has its own quota for /gscratch storage. By default, the quota is 1 TiB per slice (or 4 TiB for a GPU slice). Additional storage is available at $10/TiB/month as of 2023-12-08.\n\n\n\n\n\n\ninode limits and Apptainer\n\n\n\nAs reported by the hyakstorage utility, the /gscratch filesystem and your home directory are configured to limit the number of files that can be stored by a user or group. If you exceed this limit, you will not be able to write to the filesystem. Furthermore, because of the way that the software underlying the filesystem (GPFS) works, having or accessing a large number of files can slow down the filesystem for everyone.\nSome software is prone to creating a large number of files ‚Äì pip and conda are common culprits. If you are using software that creates a large number of files, you should consider using Apptainer to run your software in a container. The files installed by the software will be compressed and stored in a single file (the container image), which will reduce the number of files on the filesystem and potentially speed up your software.\n\n\n\n\n\nThere is also a directory /gscratch/scrubbed, which is a scratch directory where any data not accessed in 21 days is automatically deleted. This directory is slower than other directories under /gscratch. There is no quota on this directory, but you should not store data here that you cannot afford to lose.\n\n\n\n\n\n\nFinding large or old files\n\n\n\nYou can browse through the largest files and directories in any directory using the gdu command, which you can load using module load escience/gdu when logged in to a compute node.\nIf you want to look up files nobody has accessed in a while, you can use the find command. The following command will find all files in the current directory that have not been accessed in the last 14 days (atime +14) and are larger than 1 MiB (-size +1M):\nfind . -atime +14 -size +1M -printf '%AFT%AH:%AM\\t%s\\t%p\\n' | \\\n1    awk '{printf \"%s\\t%dM\\t%s\\n\", $1, $2/(1024*1024), $3}'\n\n1\n\nWe use awk to process the file size in bytes that find reports and convert it to MiB.\n\n\n\n\n\n\n\n\n\n\nhttps://hyak.uw.edu/blog/klone-storage-update\nhttps://hyak.uw.edu/docs/storage",
    "crumbs": [
      "Storage",
      "Storage on Klone"
    ]
  },
  {
    "objectID": "docs/storage/klone-storage.html#overview",
    "href": "docs/storage/klone-storage.html#overview",
    "title": "Storage on Klone",
    "section": "",
    "text": "The Klone cluster makes available several locations for storing data, some of which are shared across the login nodes and compute nodes, and some of which are only available to the node where a job is running.\n\n\nThe following storage locations are shared across the login nodes and compute nodes:\n\n\nEvery user on Klone has a home directory, which is the default location for storing files. The home directory is located at /mmfs1/home/&lt;username&gt;. The home directory is where you are when you log in to the login node, and it is also acessible by any compute node where you have a job running.\n\nNone of the storage locations on Klone are backed up. If you need to store data that you cannot afford to lose, you should use a different storage location ‚Äì see our guide to storage for more information.\n\nCurrently, there is a 10 GiB quota on the home directory. This quota is enforced by the system, and you will not be able to write to your home directory if you exceed it. You can check your quota with the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --home\n\n1\n\nThe --home flag tells hyakstorage to check your home directory quota.\n\n\nThe result will look something like this:\n                       Usage report for /mmfs1/home/altan\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 1GB / 10GB                 ‚îÇ 11579 / 256000 files        ‚îÇ\n‚îÇ                   ‚îÇ 10%                        ‚îÇ 5%                          ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nIt is important to note that the home directory is not intended for storing large amounts of data. If you need to store more than a few GiB of data, you should use one of the other storage options described below.\n\n\n\n\n\n\nDefault software installation locations\n\n\n\nA lot of software will save files to your home directory by default. Python, for example, will install packages to ~/.local/lib/python3.6 by default when they are installed using pip install --user. Apptainer will also cache images in ~/.apptainer/cache by default. Fortunately, both of these can be changed by setting environment variables or using solutions like virtual environments.\nIf you are installing software, you should always check to see where it is being installed, and make sure that it is not being installed to your home directory unless you have a good reason for doing so.\n\n\n\n\n\nEvery group on Klone has a directory under /mmfs1/gscratch (also aliased as /gscratch), which is a good place to store data that you are using for a job, as well as any data and libraries you install. The gscratch directory is accessible from any node.\nEach group has a quota on their gscratch directory, and you will not be able to write to your group‚Äôs gscratch directory if you exceed it.\nYou can find out the locataion of your group‚Äôs gscratch directory and see how much is used of your available capacity using the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --gscratch\n\n1\n\nThe --gscratch flag tells hyakstorage to check your group‚Äôs gscratch quota.\n\n\nThe result will look something like this:\n                   Usage report for /mmfs1/gscratch/escience\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 3305GB / 4096GB            ‚îÇ 710698 / 4000000 files      ‚îÇ\n‚îÇ                   ‚îÇ 81%                        ‚îÇ 18%                         ‚îÇ\n‚îÇ My usage:         ‚îÇ 41GB                       ‚îÇ 11776 files                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nThe line at the top shows the location of your group‚Äôs gscratch directory.\nEvery lab or group has its own quota for /gscratch storage. By default, the quota is 1 TiB per slice (or 4 TiB for a GPU slice). Additional storage is available at $10/TiB/month as of 2023-12-08.\n\n\n\n\n\n\ninode limits and Apptainer\n\n\n\nAs reported by the hyakstorage utility, the /gscratch filesystem and your home directory are configured to limit the number of files that can be stored by a user or group. If you exceed this limit, you will not be able to write to the filesystem. Furthermore, because of the way that the software underlying the filesystem (GPFS) works, having or accessing a large number of files can slow down the filesystem for everyone.\nSome software is prone to creating a large number of files ‚Äì pip and conda are common culprits. If you are using software that creates a large number of files, you should consider using Apptainer to run your software in a container. The files installed by the software will be compressed and stored in a single file (the container image), which will reduce the number of files on the filesystem and potentially speed up your software.\n\n\n\n\n\nThere is also a directory /gscratch/scrubbed, which is a scratch directory where any data not accessed in 21 days is automatically deleted. This directory is slower than other directories under /gscratch. There is no quota on this directory, but you should not store data here that you cannot afford to lose.\n\n\n\n\n\n\nFinding large or old files\n\n\n\nYou can browse through the largest files and directories in any directory using the gdu command, which you can load using module load escience/gdu when logged in to a compute node.\nIf you want to look up files nobody has accessed in a while, you can use the find command. The following command will find all files in the current directory that have not been accessed in the last 14 days (atime +14) and are larger than 1 MiB (-size +1M):\nfind . -atime +14 -size +1M -printf '%AFT%AH:%AM\\t%s\\t%p\\n' | \\\n1    awk '{printf \"%s\\t%dM\\t%s\\n\", $1, $2/(1024*1024), $3}'\n\n1\n\nWe use awk to process the file size in bytes that find reports and convert it to MiB.",
    "crumbs": [
      "Storage",
      "Storage on Klone"
    ]
  },
  {
    "objectID": "docs/storage/klone-storage.html#references",
    "href": "docs/storage/klone-storage.html#references",
    "title": "Storage on Klone",
    "section": "",
    "text": "https://hyak.uw.edu/blog/klone-storage-update\nhttps://hyak.uw.edu/docs/storage",
    "crumbs": [
      "Storage",
      "Storage on Klone"
    ]
  },
  {
    "objectID": "docs/storage/concepts.html",
    "href": "docs/storage/concepts.html",
    "title": "Concepts and terminology",
    "section": "",
    "text": "A number of concepts and terms are used when discussing research data storage. This page provides an overview of some of the most common concepts and terms.\n\n\nThere are many different types of storage. Some of the most common types of storage are:\n\n\nObject storage is a type of storage that stores data as objects. Each object contains data, metadata, and a unique identifier. Objects are stored in a flat address space and can be accessed using a URL. Object storage is designed to store large amounts of unstructured data. Object storage is not designed to be used as a file system. Object storage is typically accessed using a REST API. Object storage is typically used for storing data that is not actively being worked on but needs to be stored for a long time. Amazon S3 and Azure Blobs are examples of object storage services.\n\n\n\nBlock storage is a type of storage that stores data as blocks. Each block contains data and a unique identifier. Blocks are stored in a hierarchical address space and can be accessed using a file system. Block storage is designed to store large amounts of structured data. Block storage is typically used for storing data that is actively being worked on. Amazon EBS is an example of block storage.\n\n\n\nFile storage is a type of storage that stores data as files. Your computer‚Äôs local filesystem uses file storage. Network filesystems such as NFS and SMB, and cloud storage services such as Azure Files and Amazon EFS also use file storage. File storage is designed to store large amounts of structured data. File storage is designed to be used by multiple users at the same time and supports creating, reading, updating, and deleting files. File storage is typically used for storing data that is actively being worked on. Azure Files is an example of file storage.\n\n\nCloud-based file sync services, such as Dropbox, Google Drive, and OneDrive, offer file storage that can easily be accessed from multiple devices. These services usually offer convenient features such as file versioning, recovery, and file sharing. You can access the files stored in these services from a directory on your computer, and the files are automatically synchronized with the cloud storage service. However, the services may not support all of the features of a traditional file system (e.g., file permissions, symbolic links) and often have restrictions on the length of file names and the types of characters that can be used in file names, which may cause serious problems when using files stored in these services with some applications (e.g., FreeSurfer). Caution is advised when working with files stored in these services.\n\n\n\n\nVersion control systems, such as Git, are designed to store and manage different versions of files. Version control systems are typically used for storing source code but can also be used for storing other types of files. There are some version control systems that are designed to handle datasets, such as DVC. GitHub provides online hosting and collaboration services for Git repositories.\n\n\n\nData repositories, such as ResearchWorks Archive and OSF, are designed to store and manage research data and their metadata. Data repositories are typically used for storing data that is being shared with other researchers. Data repositories often provide features such as versioning, recovery, sharing, and embargos. Data repositories often provide a DOI for each dataset, which can be used to cite the dataset in publications.\n\n\n\n\nThere are a number of characteristics and considerations that are important when choosing a storage service, and these may be reflected in the upfront and ongoing costs. Some of the most common characteristics and considerations are:\n\n\nHow much data is being stored. The more data that is being stored, the more it will cost to store the data. Storage services may have minimum and/or maximum storage capacity requirements. Some services may charge for storage in increments of a certain size (e.g., 1 TiB).\n\n\n\nHow quickly data can start being read or written. The lower the latency, the faster data can be read or written. Storage services may charge more for lower latency. A solid-state drive (SSD) has lower latency than a hard disk drive (HDD), which has lower latency than a tape drive. However, storage capacity on SSDs is more expensive than storage capacity on HDDs, which is more expensive than storage capacity on tape drives.\n\n\n\nThe rate at which data can be downloaded from or uploaded to a service may be limited. The higher the bandwidth, the faster data can be downloaded from or uploaded to the service. Service providers may charge more for higher bandwidth.\nData transfer costs may be charged for uploading data to and downloading data from a service, also known as ingress and egress. Typically, ingress is free and egress is charged. Some services may charge more for uploading data to and downloading data from certain locations (e.g., outside of the United States). Some providers offer options such as physical media transfer (e.g., mailing a hard drive) for uploading and downloading large amounts of data (see AWS Snowball).\n\n\n\nHow long data needs to be stored. The longer data needs to be stored, the more it will cost to store the data. Billing for storage services is typically done on a monthly or annual basis.\n\n\n\nHow often data is backed up, how long backups are kept, and how quickly data can be recovered from backups. The more often data is backed up, the longer backups are kept, and the faster data can be recovered from backups, the more it will cost to store the data. Backup and recovery options may not be available or involve extra charges on some services.\n\n\n\nHow often data is accessed. Some services may charge for each time data is accessed or charge more for more frequent access.\n\n\n\nWho can access the data. Data access restrictions may be inherent to a service (e.g., only people connected to the UW intranet can access the data) or may be imposed by the user, the organization, or the service provider. Sharing data with outside collaborators may require additional steps, such as creating accounts for the collaborators, or may not be possible at all.\n\n\n\nHow secure the data is against unauthorized access. While access restrictions can limit who can access the data in certain circumstances, measures such as encryption may be necessary to prevent access by users who have access to the data but should not be able to access the data (e.g., system administrators).\nResearch data that is subject to HIPAA or FERPA regulations must be stored in a HIPAA- or FERPA-compliant service. For more information about HIPAA at UW, see the HIPPA Guidance page.",
    "crumbs": [
      "Storage",
      "Concepts and terminology"
    ]
  },
  {
    "objectID": "docs/storage/concepts.html#storage-types",
    "href": "docs/storage/concepts.html#storage-types",
    "title": "Concepts and terminology",
    "section": "",
    "text": "There are many different types of storage. Some of the most common types of storage are:\n\n\nObject storage is a type of storage that stores data as objects. Each object contains data, metadata, and a unique identifier. Objects are stored in a flat address space and can be accessed using a URL. Object storage is designed to store large amounts of unstructured data. Object storage is not designed to be used as a file system. Object storage is typically accessed using a REST API. Object storage is typically used for storing data that is not actively being worked on but needs to be stored for a long time. Amazon S3 and Azure Blobs are examples of object storage services.\n\n\n\nBlock storage is a type of storage that stores data as blocks. Each block contains data and a unique identifier. Blocks are stored in a hierarchical address space and can be accessed using a file system. Block storage is designed to store large amounts of structured data. Block storage is typically used for storing data that is actively being worked on. Amazon EBS is an example of block storage.\n\n\n\nFile storage is a type of storage that stores data as files. Your computer‚Äôs local filesystem uses file storage. Network filesystems such as NFS and SMB, and cloud storage services such as Azure Files and Amazon EFS also use file storage. File storage is designed to store large amounts of structured data. File storage is designed to be used by multiple users at the same time and supports creating, reading, updating, and deleting files. File storage is typically used for storing data that is actively being worked on. Azure Files is an example of file storage.\n\n\nCloud-based file sync services, such as Dropbox, Google Drive, and OneDrive, offer file storage that can easily be accessed from multiple devices. These services usually offer convenient features such as file versioning, recovery, and file sharing. You can access the files stored in these services from a directory on your computer, and the files are automatically synchronized with the cloud storage service. However, the services may not support all of the features of a traditional file system (e.g., file permissions, symbolic links) and often have restrictions on the length of file names and the types of characters that can be used in file names, which may cause serious problems when using files stored in these services with some applications (e.g., FreeSurfer). Caution is advised when working with files stored in these services.\n\n\n\n\nVersion control systems, such as Git, are designed to store and manage different versions of files. Version control systems are typically used for storing source code but can also be used for storing other types of files. There are some version control systems that are designed to handle datasets, such as DVC. GitHub provides online hosting and collaboration services for Git repositories.\n\n\n\nData repositories, such as ResearchWorks Archive and OSF, are designed to store and manage research data and their metadata. Data repositories are typically used for storing data that is being shared with other researchers. Data repositories often provide features such as versioning, recovery, sharing, and embargos. Data repositories often provide a DOI for each dataset, which can be used to cite the dataset in publications.",
    "crumbs": [
      "Storage",
      "Concepts and terminology"
    ]
  },
  {
    "objectID": "docs/storage/concepts.html#characteristics-and-considerations",
    "href": "docs/storage/concepts.html#characteristics-and-considerations",
    "title": "Concepts and terminology",
    "section": "",
    "text": "There are a number of characteristics and considerations that are important when choosing a storage service, and these may be reflected in the upfront and ongoing costs. Some of the most common characteristics and considerations are:\n\n\nHow much data is being stored. The more data that is being stored, the more it will cost to store the data. Storage services may have minimum and/or maximum storage capacity requirements. Some services may charge for storage in increments of a certain size (e.g., 1 TiB).\n\n\n\nHow quickly data can start being read or written. The lower the latency, the faster data can be read or written. Storage services may charge more for lower latency. A solid-state drive (SSD) has lower latency than a hard disk drive (HDD), which has lower latency than a tape drive. However, storage capacity on SSDs is more expensive than storage capacity on HDDs, which is more expensive than storage capacity on tape drives.\n\n\n\nThe rate at which data can be downloaded from or uploaded to a service may be limited. The higher the bandwidth, the faster data can be downloaded from or uploaded to the service. Service providers may charge more for higher bandwidth.\nData transfer costs may be charged for uploading data to and downloading data from a service, also known as ingress and egress. Typically, ingress is free and egress is charged. Some services may charge more for uploading data to and downloading data from certain locations (e.g., outside of the United States). Some providers offer options such as physical media transfer (e.g., mailing a hard drive) for uploading and downloading large amounts of data (see AWS Snowball).\n\n\n\nHow long data needs to be stored. The longer data needs to be stored, the more it will cost to store the data. Billing for storage services is typically done on a monthly or annual basis.\n\n\n\nHow often data is backed up, how long backups are kept, and how quickly data can be recovered from backups. The more often data is backed up, the longer backups are kept, and the faster data can be recovered from backups, the more it will cost to store the data. Backup and recovery options may not be available or involve extra charges on some services.\n\n\n\nHow often data is accessed. Some services may charge for each time data is accessed or charge more for more frequent access.\n\n\n\nWho can access the data. Data access restrictions may be inherent to a service (e.g., only people connected to the UW intranet can access the data) or may be imposed by the user, the organization, or the service provider. Sharing data with outside collaborators may require additional steps, such as creating accounts for the collaborators, or may not be possible at all.\n\n\n\nHow secure the data is against unauthorized access. While access restrictions can limit who can access the data in certain circumstances, measures such as encryption may be necessary to prevent access by users who have access to the data but should not be able to access the data (e.g., system administrators).\nResearch data that is subject to HIPAA or FERPA regulations must be stored in a HIPAA- or FERPA-compliant service. For more information about HIPAA at UW, see the HIPPA Guidance page.",
    "crumbs": [
      "Storage",
      "Concepts and terminology"
    ]
  },
  {
    "objectID": "docs/software/matlab.html",
    "href": "docs/software/matlab.html",
    "title": "MATLAB",
    "section": "",
    "text": "module spider matlab\n\n\n\nLoad the default version with module load matlab,\nor load a specific version with module load matlab/&lt;version&gt;.\n\n\n\nTo run Matlab with GUI (in a VNC session or with X11-Forwarding enabled), run:\nmatlab\nTo run Matlab from terminal CLI, run:\nmatlab -nodisplay",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#checking-available-versions",
    "href": "docs/software/matlab.html#checking-available-versions",
    "title": "MATLAB",
    "section": "",
    "text": "module spider matlab",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#loading-matlab",
    "href": "docs/software/matlab.html#loading-matlab",
    "title": "MATLAB",
    "section": "",
    "text": "Load the default version with module load matlab,\nor load a specific version with module load matlab/&lt;version&gt;.",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#running-matlab",
    "href": "docs/software/matlab.html#running-matlab",
    "title": "MATLAB",
    "section": "",
    "text": "To run Matlab with GUI (in a VNC session or with X11-Forwarding enabled), run:\nmatlab\nTo run Matlab from terminal CLI, run:\nmatlab -nodisplay",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/afni.html",
    "href": "docs/software/afni.html",
    "title": "AFNI",
    "section": "",
    "text": "module avail afni\n\n\n\nLoad AFNI with module load forsyth/afni.\nAfter loading the module, all AFNI tools are made accessible in the shell session.\n\n\n\n\n\nDownload AFNI installer:\n\ncurl -O https://afni.nimh.nih.gov/pub/dist/bin/misc/@update.afni.binaries\n\nInstall the CentOS 7 package to /sw/contrib/labname-src/afni (replacing mylabname with the name of your lab):\n\nTARGET=\"/sw/contrib/mylabname-src/afni/\"\ntcsh @update.afni.binaries -package linux_centos_7_64 -do_extras -bindir \"$TARGET\"\n\nInstall required R libraries to /sw/contrib/mylabname-src/afni/Rlibs:\n\nexport R_LIBS=\"$TARGET/Rlibs\"\nmkdir -p \"$R_LIBS\"\nrPkgsInstall -pkgs ALL\n\nCreate an Lmod .lua module file for AFNI with a text editor:\n\n\n\n/sw/contrib/modulefiles/mylabname/afni.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\nappend_path(\"PATH\", base)\nsetenv(\"R_LIBS\", r_libs)\n\nif (mode() == \"load\") then\n    LmodMessage(\"R_LIBS set to \" .. r_libs)\n    LmodMessage(\"------------------------------------------------------------\")\n    LmodMessage(\"On initial use, run the following:\")\n    LmodMessage(\"  suma -update_env\")\nend\n\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/afni\nmodule load mylabname/afni\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/afni.html#checking-available-versions",
    "href": "docs/software/afni.html#checking-available-versions",
    "title": "AFNI",
    "section": "",
    "text": "module avail afni",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/afni.html#using-afni-from-forsyth",
    "href": "docs/software/afni.html#using-afni-from-forsyth",
    "title": "AFNI",
    "section": "",
    "text": "Load AFNI with module load forsyth/afni.\nAfter loading the module, all AFNI tools are made accessible in the shell session.",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/afni.html#installing-afni",
    "href": "docs/software/afni.html#installing-afni",
    "title": "AFNI",
    "section": "",
    "text": "Download AFNI installer:\n\ncurl -O https://afni.nimh.nih.gov/pub/dist/bin/misc/@update.afni.binaries\n\nInstall the CentOS 7 package to /sw/contrib/labname-src/afni (replacing mylabname with the name of your lab):\n\nTARGET=\"/sw/contrib/mylabname-src/afni/\"\ntcsh @update.afni.binaries -package linux_centos_7_64 -do_extras -bindir \"$TARGET\"\n\nInstall required R libraries to /sw/contrib/mylabname-src/afni/Rlibs:\n\nexport R_LIBS=\"$TARGET/Rlibs\"\nmkdir -p \"$R_LIBS\"\nrPkgsInstall -pkgs ALL\n\nCreate an Lmod .lua module file for AFNI with a text editor:\n\n\n\n/sw/contrib/modulefiles/mylabname/afni.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\nappend_path(\"PATH\", base)\nsetenv(\"R_LIBS\", r_libs)\n\nif (mode() == \"load\") then\n    LmodMessage(\"R_LIBS set to \" .. r_libs)\n    LmodMessage(\"------------------------------------------------------------\")\n    LmodMessage(\"On initial use, run the following:\")\n    LmodMessage(\"  suma -update_env\")\nend\n\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/afni\nmodule load mylabname/afni\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/spm.html",
    "href": "docs/software/spm.html",
    "title": "SPM Toolbox",
    "section": "",
    "text": "The Statistical Parametric Mapping (SPM) toolbox for MATLAB is available as an Lmod module on Klone compute nodes.\n\n\nLike all Lmod modules, the spm module is available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the module for SPM is installed in the escience hierarchy, its name is prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/spm\nThe output should look a bit like this:\n---------------------------------------\n  escience/spm: escience/spm/12\n---------------------------------------\n\n     This module can be loaded directly: module load escience/spm/12\n\n     Help:\n        spm12\n\n\n\nLoad the default version with module load escience/spm or load a specific version with module load escience/spm/version.\nAfter loading the module, start matlab and run spm in the MATLAB command window.\n\n\n\nYou can install different versions of SPM by following the instructions below.\n\nGo to https://www.fil.ion.ucl.ac.uk/spm/software/download, choose the version of SPM you want to install, copy the download link to the file, and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 8 as an example:\n1mkdir -p \"/gscratch/scrubbed/$USER/downloads\" && cd \"$_\"\n2curl -LO https://www.fil.ion.ucl.ac.uk/spm/download/restricted/dyll/spm8.zip\n\n1\n\nCreate the target directory if it does not exist and navigate to it ($_ is a special variable that expands to the last argument of the previous command).\n\n2\n\ncurl is a program you can use to download files from the web. Replace the URL with the URL for the version of SPM you want to install. The -L option tells curl to follow redirects, and the -O option tells it to save the file to the current directory.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe directory /gscratch/scrubbed/$USER/downloads will expand to /gscratch/scrubbed/(your username)/downloads. This directory is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the archive and copy the contents to /sw/contrib/mylabname-src/spm/8 (replacing mylabname with the name of your lab):\n1unzip spm8.zip -d spm8.zip\n2# Make sure you include the parentheses around the commands below. Replace \"mylabname\" with your lab name!\n3(umask 002 && mkdir -p /sw/contrib/mylabname-src/spm/8 && rsync -rlHP --chmod=a+rwX spm8/spm8/ $_)\n4rm -rfv spm8.zip spm8\n\n1\n\nExtract the SPM archive to a new directory called spm8.\n\n2\n\nIn bash, parentheses create a [subshell]((https://tldp.org/LDP/abs/html/subshells.html), which runs the enclosed commands in a new shell session. We want to use it here so that we can temporarily modify the default permissions for new files and directories with the umask command. Your current settings will remain intact when the commands in the subshell are completed.\n\n3\n\nWe‚Äôre running a series of commands here in a [subshell]((https://tldp.org/LDP/abs/html/subshells.html). Let‚Äôs break it down:\n\n4\n\nRemove the downloaded file and the extracted directory.\n\n\nUse a text editor to create an Lmod .lua module file for the new release with a text editor, using 8.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/spm8.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\ndepends_on(\"matlab\")\nappend_path(\"MATLABPATH\", base)\n\n\n\n\n\n\n\nTip\n\n\n\nIf you don‚Äôt know how to use a text editor on Klone, you can try nano, which is installed on the system. To create a new file, run nano filename, paste the contents of the template into the file, and save it by pressing Ctrl+O and then Enter. To exit, press Ctrl+X.\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/spm\nmodule load mylabname/spm/8\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.",
    "crumbs": [
      "Software",
      "SPM Toolbox"
    ]
  },
  {
    "objectID": "docs/software/spm.html#use-with-lmod",
    "href": "docs/software/spm.html#use-with-lmod",
    "title": "SPM Toolbox",
    "section": "",
    "text": "Like all Lmod modules, the spm module is available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the module for SPM is installed in the escience hierarchy, its name is prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/spm\nThe output should look a bit like this:\n---------------------------------------\n  escience/spm: escience/spm/12\n---------------------------------------\n\n     This module can be loaded directly: module load escience/spm/12\n\n     Help:\n        spm12\n\n\n\nLoad the default version with module load escience/spm or load a specific version with module load escience/spm/version.\nAfter loading the module, start matlab and run spm in the MATLAB command window.\n\n\n\nYou can install different versions of SPM by following the instructions below.\n\nGo to https://www.fil.ion.ucl.ac.uk/spm/software/download, choose the version of SPM you want to install, copy the download link to the file, and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 8 as an example:\n1mkdir -p \"/gscratch/scrubbed/$USER/downloads\" && cd \"$_\"\n2curl -LO https://www.fil.ion.ucl.ac.uk/spm/download/restricted/dyll/spm8.zip\n\n1\n\nCreate the target directory if it does not exist and navigate to it ($_ is a special variable that expands to the last argument of the previous command).\n\n2\n\ncurl is a program you can use to download files from the web. Replace the URL with the URL for the version of SPM you want to install. The -L option tells curl to follow redirects, and the -O option tells it to save the file to the current directory.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe directory /gscratch/scrubbed/$USER/downloads will expand to /gscratch/scrubbed/(your username)/downloads. This directory is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the archive and copy the contents to /sw/contrib/mylabname-src/spm/8 (replacing mylabname with the name of your lab):\n1unzip spm8.zip -d spm8.zip\n2# Make sure you include the parentheses around the commands below. Replace \"mylabname\" with your lab name!\n3(umask 002 && mkdir -p /sw/contrib/mylabname-src/spm/8 && rsync -rlHP --chmod=a+rwX spm8/spm8/ $_)\n4rm -rfv spm8.zip spm8\n\n1\n\nExtract the SPM archive to a new directory called spm8.\n\n2\n\nIn bash, parentheses create a [subshell]((https://tldp.org/LDP/abs/html/subshells.html), which runs the enclosed commands in a new shell session. We want to use it here so that we can temporarily modify the default permissions for new files and directories with the umask command. Your current settings will remain intact when the commands in the subshell are completed.\n\n3\n\nWe‚Äôre running a series of commands here in a [subshell]((https://tldp.org/LDP/abs/html/subshells.html). Let‚Äôs break it down:\n\n4\n\nRemove the downloaded file and the extracted directory.\n\n\nUse a text editor to create an Lmod .lua module file for the new release with a text editor, using 8.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/spm8.lua\n\nhelp(myModuleName())\nlocal base = pathJoin(\n    \"/sw/contrib\",\n    string.gsub(myModuleName(), \"/.*$\", \"-src\"),\n    string.gsub(myModuleName(), \"^.*/\", \"\"),\n    myModuleVersion()\n)\nwhatis(\"Name: \" .. string.gsub(myModuleName(), \"^.*/\", \"\"))\nwhatis(\"Version: \" .. myModuleVersion())\ndepends_on(\"matlab\")\nappend_path(\"MATLABPATH\", base)\n\n\n\n\n\n\n\nTip\n\n\n\nIf you don‚Äôt know how to use a text editor on Klone, you can try nano, which is installed on the system. To create a new file, run nano filename, paste the contents of the template into the file, and save it by pressing Ctrl+O and then Enter. To exit, press Ctrl+X.\n\n\nCheck that the module is available and load it:\n1module -I spider mylabname/spm\nmodule load mylabname/spm/8\n\n1\n\nLmod takes some time to cache available modules. You can use the -I option to force Lmod to check for new modules.",
    "crumbs": [
      "Software",
      "SPM Toolbox"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html",
    "href": "docs/compute/apptainer.html",
    "title": "Apptainer",
    "section": "",
    "text": "Apptainer (formerly Singularity) is a simple container platform enabling users to install and run software that would otherwise be unsupported by the host environment.\nOn Klone, the apptainer command is available on all compute nodes, but it is not available on the login node.\nBy default, the apptainer command will use the system version of Apptainer:\nwhich apptainer # prints /usr/bin/apptainer\napptainer --version # apptainer version 1.2.4-1.el8 (as of 2023-12-05)\n\n\n\n\n\n\nLoading a different version of Apptainer using Lmod\n\n\n\n\n\nTo use a different version of Apptainer, load the appropriate Lmod module. For example, to use Apptainer version 1.1.5:\nmodule load apptainer/1.1.5\nwhich apptainer # prints /sw/apptainer/1.1.5/bin/apptainer\napptainer --version # apptainer version 1.1.5\nTo see all available versions of Apptainer, run:\nmodule -t spider apptainer\nThe default module is indicated by the tag (D) following the version. For example, apptainer/local (D) indicates that the default version is local. (The local version is the version installed on the system.)\n\n\n\n\n\n\n\n\n\nApptainer containers are designed to be portable, so by default, they do not have access to all the files on the host system. To make files available to the container, you must bind them to the container. This is done using the --bind option of the apptainer command or the APPTAINER_BINDPATH environment variable.\n\n\n\n\n\n\nDefault bind paths\n\n\n\nBy default, apptainer will make several directories available within the container by binding them to the same path in the container so that /somefolder on the host is available as /somefolder within the container.\nThese directories include:\n\n$HOME (your home directory, a.k.a. ~)\n/tmp (temporary directory ‚Äì unique to each node; contents are purged when the users‚Äô last job running on the node completes)\n$PWD (the current working directory, i.e., the directory you are in when you run apptainer)\n\nThe Klone Apptainer installation is also configured to bind several other directories to the same path in the container, including:\n\n/mmfs1 (the main filesystem)\n/scr (the scratch filesystem, same as /tmp)\n\n\n\nWe recommend setting the following binds before running a container:\nexport APPTAINER_BINDPATH=\"/gscratch\" # Make /gscratch available within the container\nTo start an Apptainer container interactively, run the following:\napptainer shell &lt;path_to_container&gt;\n\n\n\n\n\napptainer pull docker://&lt;image_name&gt;[:&lt;tag&gt;] # Pulls the image from docker registry\n\n\n\n\n\nLet‚Äôs try running a Python script using Apptainer. The release notes for Python version 3.11 say that it is ‚Äúbetween 10-60% faster than Python 3.10‚Äù. Is this true? Let‚Äôs find out! We‚Äôll write a simple Python script to test this and use Apptainer to run it on Python 3.10 and Python 3.11.\n\n\nspeedtest.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\n\n# Get the values of the environment variables M and N, or use default values:\n1n = int(os.getenv(\"N\", default=1000))  # How many numbers to join\nm = int(os.getenv(\"M\", default=100))  # How many times to run the test\n\n# Function to test:\ndef join_nums():\n    return \"-\".join([str(i) for i in range(n)])\n\n2# Print details about the test to stderr:\nprint(\n    f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\",\n    file=sys.stderr,\n)\n\n# Run the test:\n3result = timeit.timeit(join_nums, number=m)\n\n# Print the result:\n4print(result)\n\n\n1\n\nThe os.getenv function retrieves the value of an environment variable or returns a default value if the variable is not set.\n\n2\n\nHere, we use the file parameter to print(), which makes it write to the standard error stream (stderr) instead of the default standard output stream (stdout). This is useful because we want to print the result of the test to stdout so that we can save the output by redirecting it to a file, but we also want to print some information about the test to stderr so that it doesn‚Äôt get mixed up with the result.\n\n3\n\ntimeit.timeit() runs a function multiple times and returns the average time it took to run the function.\n\n4\n\nThe print function prints to the standard output stream (stdout) by default. We can redirect this to a file in bash using the &gt; operator.\n\n\n\n\n\n\n\n\nHow to paste and save a file in the terminal\n\n\n\nYou can use the nano command to create a new file and paste the contents of the file into the terminal. To do this, run nano &lt;filename&gt; in the terminal, paste the contents of the file into the terminal, and press Ctrl+X to exit. You will be asked if you want to save the file. Press Y to save the file and Enter to confirm the filename.\n\n\nNow let‚Äôs run the script using Python 3.10 and Python 3.11. For the image, we will use the python:3.10-slim and python:3.11-slim images from the official Python images.\n\n\n\n\n\n\nTagged container releases\n\n\n\nThe python:3.10-slim and python:3.11-slim images are tagged with the version of Python they contain. This means that if you pull the python:3.10-slim image today, it will always contain Python 3.10, even if Python 3.11 is released tomorrow.\nThe python:*-slim images are designed to contain only the minimal set of packages required to run Python and are much smaller than the standard Python images (~45 MiB vs ~350 MiB).\n\n\nWe‚Äôll use the apptainer exec command to run the script inside an Apptainer container:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe apptainer exec command runs a command inside an Apptainer container. The docker://python:3.10-slim argument tells Apptainer to use the python:3.10-slim image from the Docker registry. The python3 ./speedtest.py argument tells Apptainer to run the python3 command inside the container and provide it with the argument speedtest.py.\n\n\nYou should see something like:\n1Running join_nums() 1000*100 times on Python v3.10\n20.003154174002702348\n\n1\n\nThis is printed to stderr because we used print(..., file=sys.stderr) in the script.\n\n2\n\nThis is printed to stdout.\n\n\nIt probably didn‚Äôt take very long to run the script. Let‚Äôs try running it again with larger values of M and N. We can set the values of M and N by setting them as environment variables in the bash command prompt before running the script:\n1export M=1000 N=100_000\napptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe export command sets the values of the M and N environment variables and makes them available to other programs like apptainer. We‚Äôre setting N to 100_000 instead of 100000 because underscores can be used in Python to make large numbers easier to read. This is a feature of Python, not the shell (which interprets 100_000 as just a string and not a number).\n\n\nIt should take a bit longer to run this time.\nNow, let‚Äôs try running the script using Python 3.11:\napptainer exec docker://python:3.11-slim python3 speedtest.py\nWe don‚Äôt need to set the values of M and N again because they are still set from the previous command via export (unless you closed your terminal window or logged out).\nIt probably took a bit less time to run the script this time.\nIs Python 3.11 really faster than Python 3.10? Let‚Äôs find out by redirecting the output of the script to a file:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py &gt; py3.10.txt\napptainer exec docker://python:3.11-slim python3 speedtest.py &gt; py3.11.txt\n2cat py3.10.txt py3.11.txt # show the results\n\n1\n\nIn bash, the &gt; operator redirects the output of a command to a file. If the file already exists, it will be overwritten. If you want to append to an existing file instead, use the &gt;&gt; operator. These operators can be used with any command, not just apptainer. For example, ls &gt; my_files.txt will write the output of ls to the file my_files.txt.\n\n2\n\nThe cat command prints the contents of a file to&gt; stdout. If you want to print the contents of multiple files, you can list them all as arguments to cat.\n\n\nWe see the results, but they‚Äôre not very easy to interpret. Let‚Äôs pipe the output to Python to calculate the speedup:\n1cat py3.10.txt py3.11.txt | apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'\n\n1\n\nThe | operator pipes the output of one command to the input of another. In this case, we are piping the output of cat py3.10.txt py3.11.txt to the input of apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'. The -c option of the python3 command tells Python to run the code provided as an argument. The code print(float(input())/float(input())) reads two lines of input from standard input, converts them to floating point numbers, divides the first by the second, and prints the result.\n\n\nIn my case, Python 3.11 was about 1.3 times faster than Python 3.10. Not bad!\nHere‚Äôs a demo of the above commands. Note that I used M=200 and N=50_000 instead of M=1000 and N=100_000 because it takes a long time to run the script with the larger values of M and N.\n\n\n\n\n\n\nWhat if we wanted to add a command-line interface to make it possible to run the script with different values of M and N without having to set them as environment variables? A user might want to set the values of M and N as command-line arguments. Maybe they would also like to be able to specify the output file instead of redirecting the output to a file. And how about a progress bar? Users love progress bars!\nThis sounds like a tall order, but it‚Äôs quite easy to do in Python using the click package for Python. Here‚Äôs the new script:\n\n\nspeedtest-cli.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\nimport click\n\n\n# Set up the context for the command line interface and add the options:\n@click.command()\n@click.option(\n    \"-n\",  # The name of the option\n    envvar=\"N\",  # Use the environment variable `N` if it exists\n    default=1000,  # Default value if `N` is not set\n    help=\"How many numbers to join\",  # Help text for the `-n` option\n    type=int,  # Convert the value to an integer\n)\n@click.option(\n    \"-m\",\n    envvar=\"M\",\n    default=100,\n    help=\"How many times to run the test\",\n    type=int,\n)\n@click.option(\n    \"--output\",\n1    default=\"/dev/stdout\",  # Write the result to stdout by default\n    help=\"Path to the output file\",\n2    type=click.Path(writable=True, dir_okay=False),\n)\n3def speedtest(m, n, output):\n    \"\"\"Run a speed test.\"\"\"  # Help text for the command\n\n    def join_nums():  # The function to test\n        return \"-\".join([str(i) for i in range(n)])\n\n    print(f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\", file=sys.stderr)\n    bar = click.progressbar(  # Create a progress bar\n        length=n * m,\n        update_min_steps=n,  # Update the progress bar every `n` steps\n        file=sys.stderr,  # Print the progress bar to stderr\n    )\n\n    def join_nums_progress():  # Wrap the function to add the progress bar\n        result = join_nums()\n        bar.update(n)  # Increment the progress bar by `n` steps\n        return result\n\n    result = timeit.timeit(join_nums_progress, number=m)  # Run the test\n\n    bar.render_finish()  # Stop rendering the progress bar\n\n    with open(output, \"w\") as f:  # Open the output file for writing\n        print(result, file=f)  # Write the result to the output file\n\n    if output != \"/dev/stdout\":  # Show the output path if it's not stdout\n        print(f\"Result written to {output}\", file=sys.stderr)\n\n\n4if __name__ == \"__main__\":  # Run the script if it's executed directly\n    speedtest()\n\n\n1\n\nWe set the default value of --output to /dev/stdout so that the script will print the result to stdout by default.\n\n2\n\nclick provides a Path type that can be used to validate paths. The writable=True option tells click that the path must be writable. The dir_okay=False option tells click that the path must not be a directory.\n\n3\n\nThe @click.command() decorator tells click that the first function definition that follows defines a command. The @click.option() decorators add options to the command-line interface. click passes the values of these options to the function using the names of the options (lowercased and with - replaced by _).\n\n4\n\nThe __name__ == \"__main__\" check ensures that the script is only run if it is executed directly and not if it is imported as a module. This is useful if you want to use the script as a module in another script. We don‚Äôt need to do this here, but it‚Äôs a good habit to get into.\n\n\nNow, let‚Äôs try running the script with apptainer exec:\napptainer exec docker://python:3.11-slim python3 speedtest-cli.py\nYou probably got a ModuleNotFoundError because the click package is not installed in the python:3.11-slim image. If you know some Python, you might think, ‚ÄúI could probably fix this if I install the click module via pip.‚Äù And you would be right ‚Äì if you were running the script in the version of Python that‚Äôs normally installed on your computer. But we‚Äôre running the script inside an Apptainer container, so we need to install the click module inside the container. Furthermore, we‚Äôre using two different versions of Python, so we need to install the click module in the containers for both versions.\nWe‚Äôll solve this by writing an Apptainer definition file to build a custom Apptainer image that contains the click module. Here‚Äôs the definition file:\n\n\nspeedtest.def\n\n1Bootstrap: docker # Where to get the base image from.\n2From: python:{{ PY_VERSION }}-slim # Which container to use as a base image.\n\n3%arguments\n    # The version of Python to use:\n    PY_VERSION=3.10\n  \n%files\n4    speedtest-cli.py /opt/local/bin/speedtest # Copy the script to the container.\n\n5%post\n    # Create a virtual environment in /opt/venv to install our dependencies:\n6    /usr/local/bin/python -m venv /opt/venv\n\n    # Install `click` and don't cache the downloaded files:\n7    /opt/venv/bin/pip install --no-cache-dir click\n\n    # Print a message to stderr to let the user know that the installation is done:\n8    echo \"$(/opt/venv/bin/python3 --version): Done installing dependencies.\" &gt;&2\n\n    # Make the `speedtest` command executable:\n9    chmod +x /opt/local/bin/speedtest\n\n10%environment\n11    export PATH=\"/opt/local/bin:$PATH\" # Add the directory with the `speedtest` command to the PATH.\n12    export PATH=\"/opt/venv/bin:$PATH\" # Add the virtual environment to the PATH.\n\n13%runscript\n    # Run the Python script with the arguments passed to the container:\n14    speedtest \"$@\"\n\n15%test\n    # Run the speedtest command to check if it works:\n    speedtest -m 2000 -n 1000\n\n\n1\n\nEach definition file must start with a Bootstrap line that tells Apptainer where to get the base image from. In this case, we‚Äôre using the docker bootstrap, which means that we‚Äôre getting the base image from the Docker registry.\n\n2\n\nWe‚Äôre using the python:{{ PY_VERSION }} image as the base image. The { PY_VERSION } part refers to a template variable and will be replaced with the value of the PY_VERSION build argument when we build the image. This means we can use the same definition file to build images for different versions of Python.\n\n3\n\nThe definition file is split into sections. The %arguments section defines the build arguments for the image. We‚Äôre using the PY_VERSION argument to specify the version of Python to use, and we‚Äôre setting the default value to 3.10.\n\n4\n\nThe %files section defines the files to copy inside the container. We‚Äôre copying the speedtest-cli.py script to /opt/local/bin/speedtest in the container. /opt/local/bin is a good place to put scripts that you want to be able to run from anywhere in the container, but you should add the directory to the PATH environment variable if you want to be able to run the script without specifying the full path to the script. We‚Äôre removing the .py extension from the script name because we want to be able to run the script by typing speedtest instead of speedtest-cli.py.\n\n5\n\nThe %post section defines the commands to run inside the container after the base image has been downloaded. This is where you should install any dependencies that your script needs.\n\n6\n\nWe‚Äôre using the python -m venv command to create a virtual environment in /opt/venv, which is where we will install the click module.\n\n7\n\nWe run pip from the virtual environment to install the click module. We use the --no-cache-dir option to tell pip not to cache the downloaded files. This is useful because we don‚Äôt need the downloaded files after we‚Äôve installed the click module ‚Äì otherwise, they would just take up space in the built image.\n\n8\n\nWe‚Äôre using the echo command to print a message to stderr to let the user know that the installation is done. We‚Äôre using the &gt;&2 operator to redirect the output of echo to stderr instead of stdout. The $(...) syntax runs the commands within and inserts the output into a string. In this case, we‚Äôre using it to insert the output of python3 --version into the string we‚Äôre about to print with echo. This helps the user know which version of Python the built image will contain.\n\n9\n\nWe‚Äôre using the chmod command to make the speedtest command executable ‚Äì otherwise, we wouldn‚Äôt be able to run it without passing it to the python3 command.\n\n10\n\nThe %environment section defines the environment variables to set in the container. Note that the environment variables set in the %environment section are only set when the container is run. They are not set when the image is built, so they are not available in the %post section, even if you move the %environment section above the %post section.\n\n11\n\nWe‚Äôre prepending /opt/local/bin to the existing value of the PATH environment variable so that we can run the speedtest command from anywhere in the container. The PATH environment variable is used by the shell to find commands. If you want to be able to run a command without specifying the full path to the command, you need to add the directory containing the command to the PATH environment variable, separated by a colon (:).\n\n12\n\nAdding the /opt/venv/bin directory to the PATH environment variable makes it possible to run the python command from the virtual environment we created in the %post section. This is necessary because we installed the click module in the virtual environment, so we need to run the python command from the virtual environment to be able to import the click module.\n\n13\n\nThe %runscript section defines the command to run when the container is run. In this case, we‚Äôre running the speedtest command we installed in the %post section.\n\n14\n\nWe‚Äôre using the $@ special parameter to pass all the arguments passed to the container to the speedtest command ‚Äì for example, if you run apptainer exec speedtest.sif --output output.txt, the speedtest command will be run with the arguments --output output.txt.\n\n15\n\nThe %test section defines the command to run when the image is built. In this case, we‚Äôre running the speedtest command to make sure that it works.\n\n\nNow we can build the image using the apptainer build command. The first argument is the name of the image to build. The second argument is the path to the definition file:\napptainer build speedtest-py3.10.sif speedtest.def\nIt might take a little while to build the image. If everything goes according to plan, you should see the output of a test run at the end of the build process when the %test section is run.\nWhen it‚Äôs done, the image will be saved as speedtest-py3.10.sif in the current directory. This image encapsulates the script and all its dependencies, so we can run it on any system that has Apptainer installed without having to worry about whether the system has a specific version of Python or the click module installed. Furthermore, because we defined a %runscript section in the definition file, we can run the script without having to specify the python3 command.\nTo run the containerized script, we can run the apptainer run command, which is similar to apptainer exec but launches the commands in the %runscript section of the definition file instead of the command specified as an argument:\napptainer run speedtest-py3.10.sif\nYou should see a progress bar and the result of the test.\nBecause the container image is marked as an executable, we can also run it directly without having to specify the apptainer command (although it will still run using Apptainer):\n./speedtest-py3.10.sif\nTry it with some different values of M and N:\napptainer run speedtest-py3.10.sif -m 30_000 -n 1000\nNow try specifying a different output file:\napptainer run speedtest-py3.10.sif -- -m 30_000 -n 1000 --output py3.10.txt\ncat py3.10.txt # Show the results\nHow do we build the image for Python 3.11? We could copy the definition file and change PY_VERSION to 3.11, but that would be a lot of work. Instead, we can use the --build-arg option of the apptainer build command to pass the value of PY_VERSION as a build argument to the definition file:\napptainer build --build-arg PY_VERSION=3.11 speedtest-py3.11.sif speedtest.def\nNow we can run the script using the new image:\napptainer run speedtest-py3.11.sif -m 30_000 -n 1000\nWe can still use the environment variables M and N to set the values of m and n:\nexport M=30_000 N=1000\napptainer run speedtest-py3.11.sif\nThis makes it easier to run both containers with the same values of M and N without having to specify them each time.\nIf we wanted to, we could even use OUTPUT_FILE to specify the output file instead of using the --output option because we let click know that OUTPUT_FILE is an alternative for the --output argument if there is no --output option:\nexport OUTPUT_FILE=py3.11-2.txt\napptainer run speedtest-py3.11.sif\nunset OUTPUT_FILE # Unset the OUTPUT_FILE environment variable so that it doesn't affect the next command\nBecause we specified the %runscript section in the definition file, we can also execute the script directly without having to specify the apptainer command:\n./speedtest-py3.11.sif -m 30_000 -n 1000\nLet‚Äôs recreate the comparison we did earlier:\nexport M=1000 N=100_000\napptainer run speedtest-py3.10.sif --output py3.10.txt\napptainer run speedtest-py3.11.sif --output py3.11.txt\n1cat py3.10.txt py3.11.txt | apptainer exec speedtest-py3.10.sif python3 -c 'print(float(input())/float(input()))'\n\n1\n\nWe‚Äôre using apptainer exec instead of apptainer run because we want to run the python3 command instead of the command specified in the definition file.\n\n\nHere‚Äôs a demo of the above commands:\n\n\n\n\n\n\n\nApptainer caches all the images it downloads in a cache directory to avoid downloading them again. The cache can get quite large, so it‚Äôs a good idea to clear it from time to time.\nYou can see the size of the cache directory by running:\napptainer cache list\nTo clear the cache, run:\napptainer cache clean\nThat should free up some space.\n\n\n\nBy default, Apptainer stores the cache in ~/.apptainer/cache. This can be a problem if you have a small home directory (e.g., if you are using the default 10 GB quota on Klone). You can change the cache directory by setting the APPTAINER_CACHE environment variable. For example, to set the cache directory to /tmp/&lt;your-username&gt;/apptainer-cache, you can use the $USER environment variable:\nexport APPTAINER_CACHE=\"/tmp/$USER/apptainer-cache\"\nApptainer will create the cache directory if it does not already exist.\nYou‚Äôll need to set the APPTAINER_CACHE environment variable every time you want to use Apptainer, so it‚Äôs a good idea to add it to your ~/.bashrc file so that it is always set when you log in.",
    "crumbs": [
      "Using the cluster",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html#using-apptainer",
    "href": "docs/compute/apptainer.html#using-apptainer",
    "title": "Apptainer",
    "section": "",
    "text": "Apptainer containers are designed to be portable, so by default, they do not have access to all the files on the host system. To make files available to the container, you must bind them to the container. This is done using the --bind option of the apptainer command or the APPTAINER_BINDPATH environment variable.\n\n\n\n\n\n\nDefault bind paths\n\n\n\nBy default, apptainer will make several directories available within the container by binding them to the same path in the container so that /somefolder on the host is available as /somefolder within the container.\nThese directories include:\n\n$HOME (your home directory, a.k.a. ~)\n/tmp (temporary directory ‚Äì unique to each node; contents are purged when the users‚Äô last job running on the node completes)\n$PWD (the current working directory, i.e., the directory you are in when you run apptainer)\n\nThe Klone Apptainer installation is also configured to bind several other directories to the same path in the container, including:\n\n/mmfs1 (the main filesystem)\n/scr (the scratch filesystem, same as /tmp)\n\n\n\nWe recommend setting the following binds before running a container:\nexport APPTAINER_BINDPATH=\"/gscratch\" # Make /gscratch available within the container\nTo start an Apptainer container interactively, run the following:\napptainer shell &lt;path_to_container&gt;",
    "crumbs": [
      "Using the cluster",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html#pulling-an-apptainer-image-from-docker.io-registry",
    "href": "docs/compute/apptainer.html#pulling-an-apptainer-image-from-docker.io-registry",
    "title": "Apptainer",
    "section": "",
    "text": "apptainer pull docker://&lt;image_name&gt;[:&lt;tag&gt;] # Pulls the image from docker registry",
    "crumbs": [
      "Using the cluster",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html#practical-examples",
    "href": "docs/compute/apptainer.html#practical-examples",
    "title": "Apptainer",
    "section": "",
    "text": "Let‚Äôs try running a Python script using Apptainer. The release notes for Python version 3.11 say that it is ‚Äúbetween 10-60% faster than Python 3.10‚Äù. Is this true? Let‚Äôs find out! We‚Äôll write a simple Python script to test this and use Apptainer to run it on Python 3.10 and Python 3.11.\n\n\nspeedtest.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\n\n# Get the values of the environment variables M and N, or use default values:\n1n = int(os.getenv(\"N\", default=1000))  # How many numbers to join\nm = int(os.getenv(\"M\", default=100))  # How many times to run the test\n\n# Function to test:\ndef join_nums():\n    return \"-\".join([str(i) for i in range(n)])\n\n2# Print details about the test to stderr:\nprint(\n    f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\",\n    file=sys.stderr,\n)\n\n# Run the test:\n3result = timeit.timeit(join_nums, number=m)\n\n# Print the result:\n4print(result)\n\n\n1\n\nThe os.getenv function retrieves the value of an environment variable or returns a default value if the variable is not set.\n\n2\n\nHere, we use the file parameter to print(), which makes it write to the standard error stream (stderr) instead of the default standard output stream (stdout). This is useful because we want to print the result of the test to stdout so that we can save the output by redirecting it to a file, but we also want to print some information about the test to stderr so that it doesn‚Äôt get mixed up with the result.\n\n3\n\ntimeit.timeit() runs a function multiple times and returns the average time it took to run the function.\n\n4\n\nThe print function prints to the standard output stream (stdout) by default. We can redirect this to a file in bash using the &gt; operator.\n\n\n\n\n\n\n\n\nHow to paste and save a file in the terminal\n\n\n\nYou can use the nano command to create a new file and paste the contents of the file into the terminal. To do this, run nano &lt;filename&gt; in the terminal, paste the contents of the file into the terminal, and press Ctrl+X to exit. You will be asked if you want to save the file. Press Y to save the file and Enter to confirm the filename.\n\n\nNow let‚Äôs run the script using Python 3.10 and Python 3.11. For the image, we will use the python:3.10-slim and python:3.11-slim images from the official Python images.\n\n\n\n\n\n\nTagged container releases\n\n\n\nThe python:3.10-slim and python:3.11-slim images are tagged with the version of Python they contain. This means that if you pull the python:3.10-slim image today, it will always contain Python 3.10, even if Python 3.11 is released tomorrow.\nThe python:*-slim images are designed to contain only the minimal set of packages required to run Python and are much smaller than the standard Python images (~45 MiB vs ~350 MiB).\n\n\nWe‚Äôll use the apptainer exec command to run the script inside an Apptainer container:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe apptainer exec command runs a command inside an Apptainer container. The docker://python:3.10-slim argument tells Apptainer to use the python:3.10-slim image from the Docker registry. The python3 ./speedtest.py argument tells Apptainer to run the python3 command inside the container and provide it with the argument speedtest.py.\n\n\nYou should see something like:\n1Running join_nums() 1000*100 times on Python v3.10\n20.003154174002702348\n\n1\n\nThis is printed to stderr because we used print(..., file=sys.stderr) in the script.\n\n2\n\nThis is printed to stdout.\n\n\nIt probably didn‚Äôt take very long to run the script. Let‚Äôs try running it again with larger values of M and N. We can set the values of M and N by setting them as environment variables in the bash command prompt before running the script:\n1export M=1000 N=100_000\napptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe export command sets the values of the M and N environment variables and makes them available to other programs like apptainer. We‚Äôre setting N to 100_000 instead of 100000 because underscores can be used in Python to make large numbers easier to read. This is a feature of Python, not the shell (which interprets 100_000 as just a string and not a number).\n\n\nIt should take a bit longer to run this time.\nNow, let‚Äôs try running the script using Python 3.11:\napptainer exec docker://python:3.11-slim python3 speedtest.py\nWe don‚Äôt need to set the values of M and N again because they are still set from the previous command via export (unless you closed your terminal window or logged out).\nIt probably took a bit less time to run the script this time.\nIs Python 3.11 really faster than Python 3.10? Let‚Äôs find out by redirecting the output of the script to a file:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py &gt; py3.10.txt\napptainer exec docker://python:3.11-slim python3 speedtest.py &gt; py3.11.txt\n2cat py3.10.txt py3.11.txt # show the results\n\n1\n\nIn bash, the &gt; operator redirects the output of a command to a file. If the file already exists, it will be overwritten. If you want to append to an existing file instead, use the &gt;&gt; operator. These operators can be used with any command, not just apptainer. For example, ls &gt; my_files.txt will write the output of ls to the file my_files.txt.\n\n2\n\nThe cat command prints the contents of a file to&gt; stdout. If you want to print the contents of multiple files, you can list them all as arguments to cat.\n\n\nWe see the results, but they‚Äôre not very easy to interpret. Let‚Äôs pipe the output to Python to calculate the speedup:\n1cat py3.10.txt py3.11.txt | apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'\n\n1\n\nThe | operator pipes the output of one command to the input of another. In this case, we are piping the output of cat py3.10.txt py3.11.txt to the input of apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'. The -c option of the python3 command tells Python to run the code provided as an argument. The code print(float(input())/float(input())) reads two lines of input from standard input, converts them to floating point numbers, divides the first by the second, and prints the result.\n\n\nIn my case, Python 3.11 was about 1.3 times faster than Python 3.10. Not bad!\nHere‚Äôs a demo of the above commands. Note that I used M=200 and N=50_000 instead of M=1000 and N=100_000 because it takes a long time to run the script with the larger values of M and N.\n\n\n\n\n\n\nWhat if we wanted to add a command-line interface to make it possible to run the script with different values of M and N without having to set them as environment variables? A user might want to set the values of M and N as command-line arguments. Maybe they would also like to be able to specify the output file instead of redirecting the output to a file. And how about a progress bar? Users love progress bars!\nThis sounds like a tall order, but it‚Äôs quite easy to do in Python using the click package for Python. Here‚Äôs the new script:\n\n\nspeedtest-cli.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\nimport click\n\n\n# Set up the context for the command line interface and add the options:\n@click.command()\n@click.option(\n    \"-n\",  # The name of the option\n    envvar=\"N\",  # Use the environment variable `N` if it exists\n    default=1000,  # Default value if `N` is not set\n    help=\"How many numbers to join\",  # Help text for the `-n` option\n    type=int,  # Convert the value to an integer\n)\n@click.option(\n    \"-m\",\n    envvar=\"M\",\n    default=100,\n    help=\"How many times to run the test\",\n    type=int,\n)\n@click.option(\n    \"--output\",\n1    default=\"/dev/stdout\",  # Write the result to stdout by default\n    help=\"Path to the output file\",\n2    type=click.Path(writable=True, dir_okay=False),\n)\n3def speedtest(m, n, output):\n    \"\"\"Run a speed test.\"\"\"  # Help text for the command\n\n    def join_nums():  # The function to test\n        return \"-\".join([str(i) for i in range(n)])\n\n    print(f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\", file=sys.stderr)\n    bar = click.progressbar(  # Create a progress bar\n        length=n * m,\n        update_min_steps=n,  # Update the progress bar every `n` steps\n        file=sys.stderr,  # Print the progress bar to stderr\n    )\n\n    def join_nums_progress():  # Wrap the function to add the progress bar\n        result = join_nums()\n        bar.update(n)  # Increment the progress bar by `n` steps\n        return result\n\n    result = timeit.timeit(join_nums_progress, number=m)  # Run the test\n\n    bar.render_finish()  # Stop rendering the progress bar\n\n    with open(output, \"w\") as f:  # Open the output file for writing\n        print(result, file=f)  # Write the result to the output file\n\n    if output != \"/dev/stdout\":  # Show the output path if it's not stdout\n        print(f\"Result written to {output}\", file=sys.stderr)\n\n\n4if __name__ == \"__main__\":  # Run the script if it's executed directly\n    speedtest()\n\n\n1\n\nWe set the default value of --output to /dev/stdout so that the script will print the result to stdout by default.\n\n2\n\nclick provides a Path type that can be used to validate paths. The writable=True option tells click that the path must be writable. The dir_okay=False option tells click that the path must not be a directory.\n\n3\n\nThe @click.command() decorator tells click that the first function definition that follows defines a command. The @click.option() decorators add options to the command-line interface. click passes the values of these options to the function using the names of the options (lowercased and with - replaced by _).\n\n4\n\nThe __name__ == \"__main__\" check ensures that the script is only run if it is executed directly and not if it is imported as a module. This is useful if you want to use the script as a module in another script. We don‚Äôt need to do this here, but it‚Äôs a good habit to get into.\n\n\nNow, let‚Äôs try running the script with apptainer exec:\napptainer exec docker://python:3.11-slim python3 speedtest-cli.py\nYou probably got a ModuleNotFoundError because the click package is not installed in the python:3.11-slim image. If you know some Python, you might think, ‚ÄúI could probably fix this if I install the click module via pip.‚Äù And you would be right ‚Äì if you were running the script in the version of Python that‚Äôs normally installed on your computer. But we‚Äôre running the script inside an Apptainer container, so we need to install the click module inside the container. Furthermore, we‚Äôre using two different versions of Python, so we need to install the click module in the containers for both versions.\nWe‚Äôll solve this by writing an Apptainer definition file to build a custom Apptainer image that contains the click module. Here‚Äôs the definition file:\n\n\nspeedtest.def\n\n1Bootstrap: docker # Where to get the base image from.\n2From: python:{{ PY_VERSION }}-slim # Which container to use as a base image.\n\n3%arguments\n    # The version of Python to use:\n    PY_VERSION=3.10\n  \n%files\n4    speedtest-cli.py /opt/local/bin/speedtest # Copy the script to the container.\n\n5%post\n    # Create a virtual environment in /opt/venv to install our dependencies:\n6    /usr/local/bin/python -m venv /opt/venv\n\n    # Install `click` and don't cache the downloaded files:\n7    /opt/venv/bin/pip install --no-cache-dir click\n\n    # Print a message to stderr to let the user know that the installation is done:\n8    echo \"$(/opt/venv/bin/python3 --version): Done installing dependencies.\" &gt;&2\n\n    # Make the `speedtest` command executable:\n9    chmod +x /opt/local/bin/speedtest\n\n10%environment\n11    export PATH=\"/opt/local/bin:$PATH\" # Add the directory with the `speedtest` command to the PATH.\n12    export PATH=\"/opt/venv/bin:$PATH\" # Add the virtual environment to the PATH.\n\n13%runscript\n    # Run the Python script with the arguments passed to the container:\n14    speedtest \"$@\"\n\n15%test\n    # Run the speedtest command to check if it works:\n    speedtest -m 2000 -n 1000\n\n\n1\n\nEach definition file must start with a Bootstrap line that tells Apptainer where to get the base image from. In this case, we‚Äôre using the docker bootstrap, which means that we‚Äôre getting the base image from the Docker registry.\n\n2\n\nWe‚Äôre using the python:{{ PY_VERSION }} image as the base image. The { PY_VERSION } part refers to a template variable and will be replaced with the value of the PY_VERSION build argument when we build the image. This means we can use the same definition file to build images for different versions of Python.\n\n3\n\nThe definition file is split into sections. The %arguments section defines the build arguments for the image. We‚Äôre using the PY_VERSION argument to specify the version of Python to use, and we‚Äôre setting the default value to 3.10.\n\n4\n\nThe %files section defines the files to copy inside the container. We‚Äôre copying the speedtest-cli.py script to /opt/local/bin/speedtest in the container. /opt/local/bin is a good place to put scripts that you want to be able to run from anywhere in the container, but you should add the directory to the PATH environment variable if you want to be able to run the script without specifying the full path to the script. We‚Äôre removing the .py extension from the script name because we want to be able to run the script by typing speedtest instead of speedtest-cli.py.\n\n5\n\nThe %post section defines the commands to run inside the container after the base image has been downloaded. This is where you should install any dependencies that your script needs.\n\n6\n\nWe‚Äôre using the python -m venv command to create a virtual environment in /opt/venv, which is where we will install the click module.\n\n7\n\nWe run pip from the virtual environment to install the click module. We use the --no-cache-dir option to tell pip not to cache the downloaded files. This is useful because we don‚Äôt need the downloaded files after we‚Äôve installed the click module ‚Äì otherwise, they would just take up space in the built image.\n\n8\n\nWe‚Äôre using the echo command to print a message to stderr to let the user know that the installation is done. We‚Äôre using the &gt;&2 operator to redirect the output of echo to stderr instead of stdout. The $(...) syntax runs the commands within and inserts the output into a string. In this case, we‚Äôre using it to insert the output of python3 --version into the string we‚Äôre about to print with echo. This helps the user know which version of Python the built image will contain.\n\n9\n\nWe‚Äôre using the chmod command to make the speedtest command executable ‚Äì otherwise, we wouldn‚Äôt be able to run it without passing it to the python3 command.\n\n10\n\nThe %environment section defines the environment variables to set in the container. Note that the environment variables set in the %environment section are only set when the container is run. They are not set when the image is built, so they are not available in the %post section, even if you move the %environment section above the %post section.\n\n11\n\nWe‚Äôre prepending /opt/local/bin to the existing value of the PATH environment variable so that we can run the speedtest command from anywhere in the container. The PATH environment variable is used by the shell to find commands. If you want to be able to run a command without specifying the full path to the command, you need to add the directory containing the command to the PATH environment variable, separated by a colon (:).\n\n12\n\nAdding the /opt/venv/bin directory to the PATH environment variable makes it possible to run the python command from the virtual environment we created in the %post section. This is necessary because we installed the click module in the virtual environment, so we need to run the python command from the virtual environment to be able to import the click module.\n\n13\n\nThe %runscript section defines the command to run when the container is run. In this case, we‚Äôre running the speedtest command we installed in the %post section.\n\n14\n\nWe‚Äôre using the $@ special parameter to pass all the arguments passed to the container to the speedtest command ‚Äì for example, if you run apptainer exec speedtest.sif --output output.txt, the speedtest command will be run with the arguments --output output.txt.\n\n15\n\nThe %test section defines the command to run when the image is built. In this case, we‚Äôre running the speedtest command to make sure that it works.\n\n\nNow we can build the image using the apptainer build command. The first argument is the name of the image to build. The second argument is the path to the definition file:\napptainer build speedtest-py3.10.sif speedtest.def\nIt might take a little while to build the image. If everything goes according to plan, you should see the output of a test run at the end of the build process when the %test section is run.\nWhen it‚Äôs done, the image will be saved as speedtest-py3.10.sif in the current directory. This image encapsulates the script and all its dependencies, so we can run it on any system that has Apptainer installed without having to worry about whether the system has a specific version of Python or the click module installed. Furthermore, because we defined a %runscript section in the definition file, we can run the script without having to specify the python3 command.\nTo run the containerized script, we can run the apptainer run command, which is similar to apptainer exec but launches the commands in the %runscript section of the definition file instead of the command specified as an argument:\napptainer run speedtest-py3.10.sif\nYou should see a progress bar and the result of the test.\nBecause the container image is marked as an executable, we can also run it directly without having to specify the apptainer command (although it will still run using Apptainer):\n./speedtest-py3.10.sif\nTry it with some different values of M and N:\napptainer run speedtest-py3.10.sif -m 30_000 -n 1000\nNow try specifying a different output file:\napptainer run speedtest-py3.10.sif -- -m 30_000 -n 1000 --output py3.10.txt\ncat py3.10.txt # Show the results\nHow do we build the image for Python 3.11? We could copy the definition file and change PY_VERSION to 3.11, but that would be a lot of work. Instead, we can use the --build-arg option of the apptainer build command to pass the value of PY_VERSION as a build argument to the definition file:\napptainer build --build-arg PY_VERSION=3.11 speedtest-py3.11.sif speedtest.def\nNow we can run the script using the new image:\napptainer run speedtest-py3.11.sif -m 30_000 -n 1000\nWe can still use the environment variables M and N to set the values of m and n:\nexport M=30_000 N=1000\napptainer run speedtest-py3.11.sif\nThis makes it easier to run both containers with the same values of M and N without having to specify them each time.\nIf we wanted to, we could even use OUTPUT_FILE to specify the output file instead of using the --output option because we let click know that OUTPUT_FILE is an alternative for the --output argument if there is no --output option:\nexport OUTPUT_FILE=py3.11-2.txt\napptainer run speedtest-py3.11.sif\nunset OUTPUT_FILE # Unset the OUTPUT_FILE environment variable so that it doesn't affect the next command\nBecause we specified the %runscript section in the definition file, we can also execute the script directly without having to specify the apptainer command:\n./speedtest-py3.11.sif -m 30_000 -n 1000\nLet‚Äôs recreate the comparison we did earlier:\nexport M=1000 N=100_000\napptainer run speedtest-py3.10.sif --output py3.10.txt\napptainer run speedtest-py3.11.sif --output py3.11.txt\n1cat py3.10.txt py3.11.txt | apptainer exec speedtest-py3.10.sif python3 -c 'print(float(input())/float(input()))'\n\n1\n\nWe‚Äôre using apptainer exec instead of apptainer run because we want to run the python3 command instead of the command specified in the definition file.\n\n\nHere‚Äôs a demo of the above commands:\n\n\n\n\n\n\n\nApptainer caches all the images it downloads in a cache directory to avoid downloading them again. The cache can get quite large, so it‚Äôs a good idea to clear it from time to time.\nYou can see the size of the cache directory by running:\napptainer cache list\nTo clear the cache, run:\napptainer cache clean\nThat should free up some space.\n\n\n\nBy default, Apptainer stores the cache in ~/.apptainer/cache. This can be a problem if you have a small home directory (e.g., if you are using the default 10 GB quota on Klone). You can change the cache directory by setting the APPTAINER_CACHE environment variable. For example, to set the cache directory to /tmp/&lt;your-username&gt;/apptainer-cache, you can use the $USER environment variable:\nexport APPTAINER_CACHE=\"/tmp/$USER/apptainer-cache\"\nApptainer will create the cache directory if it does not already exist.\nYou‚Äôll need to set the APPTAINER_CACHE environment variable every time you want to use Apptainer, so it‚Äôs a good idea to add it to your ~/.bashrc file so that it is always set when you log in.",
    "crumbs": [
      "Using the cluster",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html",
    "href": "docs/compute/slurm.html",
    "title": "SLURM basics",
    "section": "",
    "text": "The Klone cluster uses the SLURM job scheduler to manage access to compute resources and schedule user-submitted jobs.\nUsers can submit a batch job for scripted parallel tasks or an interactive job for manual tasks or GUI programs.\n\n\nUse groups command to see which groups you are a member of.\nUse hyakalloc command to check availability of compute resources.\n\n\n\nsalloc -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS\nAdd --no-shell flag if you plan to enter/exit the node without losing the job allocation.\nUse scancel &lt;job_id&gt; to terminate the job allocation.\nUse squeue --me to check information about active or pending job allocations (including job ID).\n\n\n\nFrom the login node, run the following:\nsrun -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS --pty /bin/bash\n\n\n\n\nsbatch submits a batch script to Slurm. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with ‚Äú#SBATCH‚Äù before any executable commands in the script. sbatch will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script.\n\n\n\nmybatch.job\n\n#!/usr/bin/env bash\n#\n# This is a script that the SLURM scheduler will use to run your job.\n#\n# The line at the top of the file is called a shebang.\n# It tells the shell what program to use to interpret the script.\n# In this case, we're using bash, the most common shell on Linux systems.\n# The shell is a program that reads commands from the terminal and executes them.\n# It's similar to the command prompt on Windows systems.\n#\n# Lines that begin with # are comments. They are not executed by the shell.\n# They are intended to document what the script does.\n#\n# Lines that begin with #SBATCH are special directives to the SLURM scheduler.\n# These directives tell the scheduler how to run your job.\n# They must come before any executable commands in your script.\n# **Remove a single # character from the start of the line to enable a directive.**\n#\n# For more information about SLURM directives, see the SLURM documentation:\n# https://slurm.schedmd.com/sbatch.html\n#\n# For more information about bash scripting in general, see:\n# https://www.gnu.org/software/bash/manual/bash.html\n#\n#\n# # JOB OPTIONS\n# You can specify some options here to customize your job.\n# For example, you can give your job a name with the --job-name option,\n# or specify the maximum amount of time the job can run with the --time option.\n#\n#\n# ## JOB NAME\n# The job name will appear when querying running jobs on the system and in log files.\n# The default is the name of this file.\n##SBATCH --job-name=myjob\n#\n#\n# ## RESOURCES\n# SLURM jobs run on compute nodes that are allocated to your job.\n# The resources available to your job depend on the account you are using,\n# the partition you request, and the number of nodes and tasks you request.\n# To see the accounts and partitions available to you, run the command `hyakalloc`.\n#\n##SBATCH --account=&lt;lab&gt; # Replace &lt;lab&gt; with your lab's name, e.g., \"psych\".\n##SBATCH --partition=&lt;partition&gt; # Replace &lt;partition&gt; with the name of the partition you want to use, e.g., \"cpu-g2-mem2x\".\n##SBATCH --nodes=&lt;number&gt; # Number of nodes to allocate\n##SBATCH --ntasks-per-node=&lt;number&gt; # Number of cores to allocate on each node\n##SBATCH --mem=&lt;number&gt;[units] # How much memory to allocate per node, with units (M|G|T), e.g., 10G = 10 gigabytes\n##SBATCH --gpus=&lt;number&gt; # Number of GPUs to allocate, e.g., 1\n##SBATCH --time=&lt;time&gt; # Max runtime in DD-HH:MM:SS format, e.g., 02-12:00:00 = 1 day, 12 hours\n#\n#\n# ## EMAIL NOTIFICATIONS\n# SLURM can send email notifications when certain events relating to your job occur.\n# To enable email notifications, uncomment the following lines and replace &lt;status&gt; and &lt;email&gt; with appropriate values:\n#\n##SBATCH --mail-type=ALL # Will e-mail about all job state changes\n##SBATCH --mail-user=\"${USER}@uw.edu\" # Who to send e-mail to (here, your username at uw.edu)\n#\n# Some valid --mail-type values besides ALL include:\n#   BEGIN (job started), END (job finished), FAIL (job failed), REQUEUE (job requeued),\n#   STAGE_OUT (stage out completed), TIME_LIMIT (reached max run time), TIME_LIMIT_50 (reached 50% of max run time),\n#   TIME_LIMIT_80 (reached 80% of max run time), TIME_LIMIT_90 (reached 90% of max run time)\n# See sbatch documentation (command: `man sbatch`) for more details.\n#\n# ## WORKING DIRECTORY\n# The --chdir option tells SLURM to change to the specified directory before running your job.\n# The default is to run your job from the current working directory.\n##SBATCH --chdir=&lt;directory&gt;\n#\n#\n# ## JOB OUTPUT STREAMS\n# Processes on Unix/Linux systems write messages to two places:\n# - standard output (stdout): where the program writes its normal output\n# - standard error (stderr): where the program usually writes error and diagnostic messages\n#\n# The default behavior of SLURM is to merge these two streams into a single file,\n# which is written to the current working directory under the name 'slurm-&lt;jobid&gt;.out'.\n# This can be changed with the --output and --error options.\n#\n# To disable an output stream, use /dev/null as the file name -- e.g. --error=/dev/null\n# /dev/null is a special file that discards all data written to it.\n#\n# Uncomment the following lines to customize output and error streams:\n##SBATCH --output=output.log # Where to direct standard output (in this case, to the file 'output.log')\n##SBATCH --error=error.log # Where to direct standard error (in this case, to the file 'error.log')\n#\n#\n# Your program goes here:\necho \"Starting program\"\n&lt;myprogram&gt;\n\nsbatch mybatch.job",
    "crumbs": [
      "Using the cluster",
      "SLURM basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#checking-access-to-compute-resources",
    "href": "docs/compute/slurm.html#checking-access-to-compute-resources",
    "title": "SLURM basics",
    "section": "",
    "text": "Use groups command to see which groups you are a member of.\nUse hyakalloc command to check availability of compute resources.",
    "crumbs": [
      "Using the cluster",
      "SLURM basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#submitting-an-interactive-job-with-salloc",
    "href": "docs/compute/slurm.html#submitting-an-interactive-job-with-salloc",
    "title": "SLURM basics",
    "section": "",
    "text": "salloc -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS\nAdd --no-shell flag if you plan to enter/exit the node without losing the job allocation.\nUse scancel &lt;job_id&gt; to terminate the job allocation.\nUse squeue --me to check information about active or pending job allocations (including job ID).",
    "crumbs": [
      "Using the cluster",
      "SLURM basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#submitting-an-interactive-job-with-srun",
    "href": "docs/compute/slurm.html#submitting-an-interactive-job-with-srun",
    "title": "SLURM basics",
    "section": "",
    "text": "From the login node, run the following:\nsrun -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS --pty /bin/bash",
    "crumbs": [
      "Using the cluster",
      "SLURM basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#submitting-a-batch-job",
    "href": "docs/compute/slurm.html#submitting-a-batch-job",
    "title": "SLURM basics",
    "section": "",
    "text": "sbatch submits a batch script to Slurm. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with ‚Äú#SBATCH‚Äù before any executable commands in the script. sbatch will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script.\n\n\n\nmybatch.job\n\n#!/usr/bin/env bash\n#\n# This is a script that the SLURM scheduler will use to run your job.\n#\n# The line at the top of the file is called a shebang.\n# It tells the shell what program to use to interpret the script.\n# In this case, we're using bash, the most common shell on Linux systems.\n# The shell is a program that reads commands from the terminal and executes them.\n# It's similar to the command prompt on Windows systems.\n#\n# Lines that begin with # are comments. They are not executed by the shell.\n# They are intended to document what the script does.\n#\n# Lines that begin with #SBATCH are special directives to the SLURM scheduler.\n# These directives tell the scheduler how to run your job.\n# They must come before any executable commands in your script.\n# **Remove a single # character from the start of the line to enable a directive.**\n#\n# For more information about SLURM directives, see the SLURM documentation:\n# https://slurm.schedmd.com/sbatch.html\n#\n# For more information about bash scripting in general, see:\n# https://www.gnu.org/software/bash/manual/bash.html\n#\n#\n# # JOB OPTIONS\n# You can specify some options here to customize your job.\n# For example, you can give your job a name with the --job-name option,\n# or specify the maximum amount of time the job can run with the --time option.\n#\n#\n# ## JOB NAME\n# The job name will appear when querying running jobs on the system and in log files.\n# The default is the name of this file.\n##SBATCH --job-name=myjob\n#\n#\n# ## RESOURCES\n# SLURM jobs run on compute nodes that are allocated to your job.\n# The resources available to your job depend on the account you are using,\n# the partition you request, and the number of nodes and tasks you request.\n# To see the accounts and partitions available to you, run the command `hyakalloc`.\n#\n##SBATCH --account=&lt;lab&gt; # Replace &lt;lab&gt; with your lab's name, e.g., \"psych\".\n##SBATCH --partition=&lt;partition&gt; # Replace &lt;partition&gt; with the name of the partition you want to use, e.g., \"cpu-g2-mem2x\".\n##SBATCH --nodes=&lt;number&gt; # Number of nodes to allocate\n##SBATCH --ntasks-per-node=&lt;number&gt; # Number of cores to allocate on each node\n##SBATCH --mem=&lt;number&gt;[units] # How much memory to allocate per node, with units (M|G|T), e.g., 10G = 10 gigabytes\n##SBATCH --gpus=&lt;number&gt; # Number of GPUs to allocate, e.g., 1\n##SBATCH --time=&lt;time&gt; # Max runtime in DD-HH:MM:SS format, e.g., 02-12:00:00 = 1 day, 12 hours\n#\n#\n# ## EMAIL NOTIFICATIONS\n# SLURM can send email notifications when certain events relating to your job occur.\n# To enable email notifications, uncomment the following lines and replace &lt;status&gt; and &lt;email&gt; with appropriate values:\n#\n##SBATCH --mail-type=ALL # Will e-mail about all job state changes\n##SBATCH --mail-user=\"${USER}@uw.edu\" # Who to send e-mail to (here, your username at uw.edu)\n#\n# Some valid --mail-type values besides ALL include:\n#   BEGIN (job started), END (job finished), FAIL (job failed), REQUEUE (job requeued),\n#   STAGE_OUT (stage out completed), TIME_LIMIT (reached max run time), TIME_LIMIT_50 (reached 50% of max run time),\n#   TIME_LIMIT_80 (reached 80% of max run time), TIME_LIMIT_90 (reached 90% of max run time)\n# See sbatch documentation (command: `man sbatch`) for more details.\n#\n# ## WORKING DIRECTORY\n# The --chdir option tells SLURM to change to the specified directory before running your job.\n# The default is to run your job from the current working directory.\n##SBATCH --chdir=&lt;directory&gt;\n#\n#\n# ## JOB OUTPUT STREAMS\n# Processes on Unix/Linux systems write messages to two places:\n# - standard output (stdout): where the program writes its normal output\n# - standard error (stderr): where the program usually writes error and diagnostic messages\n#\n# The default behavior of SLURM is to merge these two streams into a single file,\n# which is written to the current working directory under the name 'slurm-&lt;jobid&gt;.out'.\n# This can be changed with the --output and --error options.\n#\n# To disable an output stream, use /dev/null as the file name -- e.g. --error=/dev/null\n# /dev/null is a special file that discards all data written to it.\n#\n# Uncomment the following lines to customize output and error streams:\n##SBATCH --output=output.log # Where to direct standard output (in this case, to the file 'output.log')\n##SBATCH --error=error.log # Where to direct standard error (in this case, to the file 'error.log')\n#\n#\n# Your program goes here:\necho \"Starting program\"\n&lt;myprogram&gt;\n\nsbatch mybatch.job",
    "crumbs": [
      "Using the cluster",
      "SLURM basics"
    ]
  }
]